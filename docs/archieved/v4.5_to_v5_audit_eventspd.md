# EventSPD Formula Audit Report

**Date:** 2026-02-15
**Scope:** Every formula in Sections A (Precompute), B (Per-Query), and 5 (Training) of `research_plan_eventspd_v4_claude.md`
**Goal:** Verify correctness, reference support, logical coherence, constant choices, and redundancy.

---

## Audit Methodology

For each formula, we evaluate:
1. **Correctness:** Is the formula written correctly? Any ambiguity or misunderstanding risk?
2. **Reference support:** Is this technique backed by published work? If novel, does it contradict known findings?
3. **Logical meaning:** What does this formula accomplish in the pipeline? Is it redundant with another formula?
4. **Contribution:** Does this formula justify its existence in the architecture?
5. **Constants:** Are dimension/sampling values properly chosen?

Findings are categorized as:
- **[BUG]** — Incorrect formula or logic error that will cause implementation failure or factual error
- **[CONCERN]** — Design choice that may hurt performance or contradicts literature
- **[AMBIGUITY]** — Formula is correct but could be misunderstood during implementation
- **[SUGGESTION]** — Improvement opportunity that strengthens the design
- **[OK]** — Formula is correct and well-supported

All findings are backed by references to published papers or verified codebases. No claims are fabricated.

---

## Summary of Findings

| # | Location | Severity | Summary |
|---|----------|----------|---------|
| F01 | A2 symbol table | AMBIGUITY | Symbol table says "strided conv" but formula uses AvgPool |
| F02 | A3 latent bank | CONCERN | Redundant Proj 128→128 on already-projected features |
| F03 | B1.1 bilinear | AMBIGUITY | `q/s_ℓ` needs explicit [-1,1] normalization for grid_sample |
| F04 | B1.2 offset PE | CONCERN | B3 claims "same Fourier encoding" but input ranges differ drastically |
| F05 | B1.4 center MLP | CONCERN | 544→128 bottleneck is aggressive; each scale gets ~25 effective dims |
| F06 | B1.5 routing token | CONCERN | Uses only level-1 features; textureless queries may route poorly |
| F07 | B2.3 / B3.1 | AMBIGUITY | Symbol **c_r** (bold=position) vs c_r (non-bold=content) overloading |
| F08 | B3.2 offset bound | BUG | False claim: "matches Deformable DETR's practice" — Deformable DETR uses unbounded offsets |
| F09 | B3.3 padding mode | BUG | False claim: GridSample_reflect is "proven better in Deformable DETR" — all implementations use `'zeros'` |
| F10 | B3.3 coordinate | AMBIGUITY | c_r^(ℓ) never defined; need explicit c_r^(ℓ) = c_r / s_ℓ |
| F11 | B3.4 aggregation | CONCERN | No true multi-head structure; heads are just extra samples (departs from Deformable DETR) |
| F12 | B3.4 softmax scope | CONCERN | Softmax over all (h,ℓ,m) jointly; Deformable DETR uses per-head softmax |
| F13 | B4.1 normalization | CONCERN | l_q and c̄_q may not be normalized; heterogeneous T_q scales could hurt attention |
| F14 | B4.2 KV norm | SUGGESTION | SAM normalizes KV tokens; add LN_kv on T_q for heterogeneous sources |
| F15 | B5.1 hidden dim | AMBIGUITY | Depth head hidden dimension not specified (should be 64) |
| F16 | B5.1 terminology | AMBIGUITY | "Relative disparity" is non-standard; should be "relative depth code" |
| F17 | B5.2 calibration | AMBIGUITY | Does r_q^ctr get calibrated by (s_t, b_t) before L_ctr? Unspecified. |
| F18 | 5.1 L_point | AMBIGUITY | ρ̂(q) undefined — is it pre-softplus (ρ_q) or post-softplus (softplus(ρ_q))? |
| F19 | 5.1 L_silog | SUGGESTION | Missing sqrt() — F^3, BTS, AdaBins, DAv2 all use sqrt(SiLog). **v5: fixed** — added sqrt() to match convention. |
| F20 | 5.1 λ_var | SUGGESTION | F^3 uses λ=0.5 (not 0.85); 0.85 from BTS is valid but should be justified. **v5: fixed** — changed to λ_var=0.5 (F^3 default), with ablation {0.5, 0.85, 1.0}. |

**Severity counts:** 2 BUGs, 6 CONCERNs, 7 AMBIGUITIEs, 3 SUGGESTIONs, 2 OKs (in the detailed findings below)

---

## Phase A: Precompute Once Per Event Window

### A1. Backbone Encoding (Line 124-126)

**Formula:** $F_t = \mathcal{F}_{\text{F}^3}(E_t)$

**[OK]** — Straightforward backbone application. F^3 outputs 32-channel features at full resolution (1280×720 with ds1 config). This is the frozen/fine-tuned backbone — no design choice to audit.

---

### A2. Feature Pyramid (Lines 131-138)

**Formula:**
$$F_t^{(1)} = \text{Proj}_{1\times1}(F_t), \quad F_t^{(2)} = \text{Proj}_{1\times1}(\text{AvgPool}_{2\times2}(F_t))$$
$$F_t^{(3)} = \text{Proj}_{1\times1}(\text{AvgPool}_{4\times4}(F_t)), \quad F_t^{(4)} = \text{Proj}_{1\times1}(\text{AvgPool}_{8\times8}(F_t))$$

#### Finding F01 [AMBIGUITY] — Symbol table inconsistency

The **symbol table** (line 93) says: `"Independent strided conv + proj from F_t"` but the **actual formula** uses `AvgPool + Proj_{1×1}`. A strided convolution learns the downsampling kernel, while AvgPool is a fixed operation. These are different architectures:

- Strided conv: `Conv2d(32, 128, kernel_size=3, stride=s)` — learned downsampling
- AvgPool + 1×1 Proj: `Conv2d(32, 128, 1)(AvgPool2d(s))` — fixed pooling + learned channel projection

The formula (AvgPool) is the correct design per the ViTDet rationale. The symbol table should match.

**Additionally:** ViTDet (Li et al., 2022) actually uses **MaxPool** for its downsampling (2×2 max pool for stride-16→32), not AvgPool. The plan's choice of AvgPool is a valid alternative (smoother), but this difference should be noted. Ablation between AvgPool and MaxPool is recommended.

**Fix:** Update symbol table row to `"Independent AvgPool + 1×1 Proj from F_t"`.

#### Formulas are otherwise correct

- 4 independent projections 32→128. Parameters: 4 × (32×128 + 128) ≈ 17K. ✓
- Spatial dimensions: 1280×720 → 640×360 → 320×180 → 160×90. All clean divisions. ✓
- ViTDet independence principle correctly applied. ✓

---

### A3. Compact Latent Bank (Lines 149-157)

**Formula:** $C_t = \text{Proj}(\text{AdaptiveAvgPool}_{P_h \times P_w}(F_t^{(3)})) \in \mathbb{R}^{P_c \times d}$

#### Finding F02 [CONCERN] — Potentially redundant projection

$F_t^{(3)}$ already has $d = 128$ channels (from the A2 projection). The AdaptiveAvgPool gives a $16\times16\times128$ tensor, reshaped to $256\times128$. The Proj then maps $128 \to 128$.

**Is this projection necessary?** If $F_t^{(3)}$ is already 128-dim, the pooled version is also 128-dim, and another 128→128 projection is a parameterized identity-like transform that adds ~17K parameters.

**Arguments for keeping it:**
- Re-parameterization after spatial pooling can help (pooled features have different statistics than the pre-pooled features)
- DETR applies a linear projection to encoder output before decoder attention
- Separates the pyramid's representation space from the latent bank's representation space

**Arguments for removing it:**
- 128→128 linear is close to identity; the cross-attention in B2 already has its own KV projections ($W_K, W_V$) that can absorb any needed re-parameterization
- Saves 17K parameters (2% of total)

**Recommendation:** Keep but note as an ablation option. If kept, clarify its purpose in the document: "This projection re-parameterizes the spatially-pooled features into the latent bank's representation space, separate from the pyramid's features."

---

### A4. Global Calibration Heads (Lines 185-189)

**Formula:**
$$s_t = \text{softplus}(h_s(\text{MeanPool}(C_t))), \quad b_t = h_b(\text{MeanPool}(C_t))$$

**[OK]** — Correct and well-supported.

- MeanPool over 256 tokens → $\mathbb{R}^{128}$. $h_s, h_b$ are single linear layers: $128 \to 1$. ✓
- softplus ensures $s_t > 0$ (positive scale). $b_t$ is unconstrained (shift can be any value). ✓
- Follows MiDaS/ZoeDepth pattern of global scene-level calibration. ✓
- MeanPool is appropriate for global aggregation (loses spatial info, keeps global statistics). ✓

**Minor note:** $s_t$ and $b_t$ are derived solely from $F_t^{(3)}$ (via $C_t$). They don't see level-1, level-2, or level-4 features. This is acceptable because $C_t$ covers the full spatial extent and level-3 already has broad context. ZoeDepth also uses global average pooling for its scene-level estimation.

---

## Phase B: Per-Query Sparse Depth Inference

### B1. Feature Extraction and Token Construction

#### B1.1 Multi-scale center features (Lines 206-208)

**Formula:** $f_q^{(\ell)} = \text{Bilinear}(F_t^{(\ell)}, q / s_\ell), \quad \ell = 1, 2, 3, 4$

#### Finding F03 [AMBIGUITY] — Missing coordinate normalization

The formula $q / s_\ell$ maps pixel coordinates to level-$\ell$ coordinates. However, PyTorch's `F.grid_sample` expects coordinates in $[-1, 1]$ (not pixel coordinates). The formula is conceptually correct but will cause implementation errors if taken literally.

**Evidence:** The F^3 codebase uses `align_corners=True` with normalization `2 * px / (W-1) - 1` (optical flow code). The Deformable DETR/DCNv3 implementations use `align_corners=False` with `2 * px / W - 1` (verified in InternImage codebase).

**Fix:** Either:
1. Add `Normalize()` wrapper: $f_q^{(\ell)} = \text{Bilinear}(F_t^{(\ell)}, \text{Normalize}(q / s_\ell))$ (consistent with B3.3 which already uses Normalize)
2. Add an implementation note: "All bilinear lookups normalize pixel coordinates to $[-1, 1]$ via $\text{grid} = 2 \cdot p / \text{dim} - 1$ with `align_corners=False`."

**Recommendation:** Use `align_corners=False` throughout (matches Deformable DETR convention). Specify this once as a global implementation note.

#### Also note: $s_\ell$ is never defined in the symbol table

$s_1 = 1, s_2 = 2, s_3 = 4, s_4 = 8$ are implied but not stated. Add to symbol table.

---

#### B1.2 Local neighborhood sampling (Lines 214-238)

**Fixed grid:** $\Omega_{\text{fixed}} = \{(\delta_x, \delta_y) : \delta_x, \delta_y \in \{-2,-1,0,1,2\}\} \setminus \{(0,0)\}, \quad |\Omega_{\text{fixed}}| = 24$ ✓

**Learned offsets:** $\Delta_m = r_{\max} \cdot \tanh(W_{\text{off}}^{(m)} f_q^{(1)} + b_{\text{off}}^{(m)}), \quad m = 1, \ldots, 8$

Per-m notation is conceptually correct. In practice, implemented as one `nn.Linear(128, 16)` with output reshaped to $(8, 2)$. ✓

**Per-sample transform:** $h_\delta = \text{GELU}(W_{\text{loc}} [f_\delta; \phi(\delta)] + b_{\text{loc}})$

Input: $f_\delta \in \mathbb{R}^{128}$ + $\phi(\delta) \in \mathbb{R}^{16}$ = 144-dim. $W_{\text{loc}}$: $144 \to 128$. Shared across all 32 samples (PointNet-style). ✓

**Aggregate:** $l_q = W_{\text{agg}} \cdot \text{MaxPool}(\{h_\delta\}_\delta) + b_{\text{agg}}$

MaxPool over 32 vectors → $\mathbb{R}^{128}$. Linear $128 \to 128$. ✓

#### Finding F04 [CONCERN] — Fourier encoding frequency mismatch between B1.2 and B3

The plan states (line 234): $\phi(\delta)$ uses "4 frequencies × 2 trig × 2 dims = 16 dims."

In B3 (line 341), the plan says: "$\phi(\cdot)$ is the **same** Fourier positional encoding."

**But the input ranges are drastically different:**
- B1.2: $\delta$ is bounded by $r_{\max} = 6$ pixels (for learned) or $\sqrt{8} \approx 2.83$ (for fixed grid). Small offsets.
- B3: $\Delta\mathbf{c}_r = \mathbf{c}_r - q$ is in original pixel coordinates. For a routed anchor across the image, this could be up to 1280 pixels.

If the same frequencies $\sigma_l = 2^l$ ($l = 0, 1, 2, 3$) are applied to raw pixel offsets:
- Highest frequency argument: $2\pi \cdot 8 \cdot 1280 = 64{,}339$ → meaningless oscillation
- Even lowest frequency: $2\pi \cdot 1 \cdot 1280 = 8{,}042$ → still too fast

**This is a real problem.** The Fourier encoding will produce essentially random values for large pixel offsets, providing no useful positional information to the offset prediction MLP.

**Fix options:**
1. **Normalize the offset in B3:** $\phi(\Delta\mathbf{c}_r / D_{\max})$ where $D_{\max}$ is the image diagonal ($\approx 1469$ for 1280×720). This maps to $[0, 1]$ range. Use pe_q's 8-frequency encoding.
2. **Use different frequencies for B3:** Choose frequencies appropriate for the expected offset range (e.g., $\sigma_l = 2^l / 1280$ for the x-component).
3. **State explicitly that B3 normalizes offsets before PE.** The text says "original pixel coordinates" for the offset — this normalization must be documented.

**Evidence:** NeRF (Mildenhall et al., 2020) and Tancik et al. (2020) both apply Fourier features to **normalized** coordinates in $[0, 1]$ or $[-1, 1]$, never to raw pixel coordinates.

---

#### B1.3 Positional encoding (Lines 246-250)

**Formula:** $\text{pe}_q = [\sin(2\pi \sigma_l u/W); \cos(2\pi \sigma_l u/W); \sin(2\pi \sigma_l v/H); \cos(2\pi \sigma_l v/H)]_{l=0}^{L_{\text{pe}}-1}$

**[OK]** — Correct. Verified against Tancik et al. (2020) "Fourier Features Let Networks Learn High Frequency Functions."

- Follows Tancik convention: $\sin(2\pi \cdot 2^l \cdot x)$ with $x = u/W \in [0, 1]$. ✓
- Apparent factor-of-2 difference vs NeRF ($\sin(2^l \pi p)$) is absorbed by NeRF's $p \in [-1, 1]$ vs Tancik's $x \in [0, 1]$. Same effective frequency at $l=0$: one full cycle across the image. ✓
- 8 frequencies give periods: 1280, 640, 320, 160, 80, 40, 20, 10 pixels. Finest frequency resolves 10-pixel features — sufficient for a "smooth address fingerprint" when combined with spatially-varying backbone features. ✓
- $d_{\text{pe}} = 32$ (8 freq × 2 trig × 2 dims). ✓

---

#### B1.4 Center token (Lines 255-258)

**Formula:** $h_{\text{point}} = \text{LN}(W_{p2} \cdot \text{GELU}(W_{p1} [f_q^{(1)}; f_q^{(2)}; f_q^{(3)}; f_q^{(4)}; \text{pe}_q] + b_{p1}) + b_{p2})$

#### Finding F05 [CONCERN] — Aggressive 544→128 bottleneck

Input: $4 \times 128 + 32 = 544$. Hidden: $128$. Output: $128$.

The first layer ($W_{p1}: 544 \to 128$) compresses 4 scales of 128-dim features + 32-dim PE into 128 dimensions. This is a 4.25× compression in one step. Each scale effectively gets ~25 dimensions of representation capacity.

**Why this matters:** The 4 scales carry qualitatively different information (fine edges at level 1, structural elements at levels 2-3, scene layout at level 4). A single linear layer must learn to mix and compress all of this simultaneously. If the information from different scales is not highly correlated, significant information loss is possible.

**Comparison:**
- DPT's reassemble blocks use 256-dim hidden layers for feature fusion
- DepthAnythingV2's depth head (for the vitl variant) goes 256→128→32→1 (gradual compression)
- Standard practice for multi-scale fusion is to use wider hidden layers at the initial mixing stage

**Recommendation:** Consider increasing hidden dim to 256 or using a 3-layer MLP (544→256→128→128). Parameters: 544×256 + 256×128 = ~172K (vs current ~70K). This is a modest increase that may improve multi-scale representation quality. **Add to ablation plan.**

**Note:** The "no skip connection" justification is correct — depth is a nonlinear function of appearance features, and forcing full nonlinear encoding is sound. The concern is purely about hidden width.

---

#### B1.5 Routing token (Lines 266-268)

**Formula:** $z_q = \text{LN}(W_z [f_q^{(1)}; l_q; \text{pe}_q] + b_z), \quad z_q \in \mathbb{R}^d$

#### Finding F06 [CONCERN] — Routing uses only level-1 features

$z_q$ drives both global retrieval (B2) and deformable conditioning (B3). Its inputs are:
- $f_q^{(1)}$: level-1 (finest) features — local appearance
- $l_q$: local neighborhood aggregate — local context
- $\text{pe}_q$: positional encoding — location

**Missing: multi-scale information.** The routing token has no access to coarser features ($f_q^{(2)}, f_q^{(3)}, f_q^{(4)}$). For a query in a **textureless region** (e.g., flat wall, sky), level-1 features may be uninformative — all nearby pixels look similar. Level-3 or level-4 features provide broader context that could produce better routing decisions (e.g., "this region is sky, route to horizon anchors").

**Why this might be intentional:** Routing should be based on "what is this query?" (content-based), not "what depth should it have?" (task-based). Using only fine-grained features prevents routing from overfitting to depth-correlated features.

**Recommendation:** This is a valid design tension. Note as an ablation option: test routing from $[f_q^{(1)}; f_q^{(3)}; l_q; \text{pe}_q]$ (adding level-3 for broader context) vs the current level-1-only approach.

**Also:** $z_q$ is a **single linear layer** + LN (no nonlinearity). This means $z_q$ is a linear function of its inputs (LN normalizes but doesn't add nonlinearity). Since $z_q$ goes through further projections ($W_Q$, $W_r$, $W_u$) in B2/B3, the effective depth is still 2+ linear layers before any decision is made. This is adequate for routing (which is fundamentally a similarity computation).

---

### B2. Global Context Retrieval

#### B2.1 Global summary (Lines 275-278)

**Formula:** $\bar{c}_q = \text{MHCrossAttn}(Q = W_Q z_q, \; K = W_K C_t, \; V = W_V C_t)$

**[OK]** — Standard multi-head cross-attention. 4 heads, $d_{\text{head}} = 32$. Attention matrix is $1 \times 256$ per head — computationally trivial. ✓

**Implicit:** The formula implies an output projection $W_O$ (concat heads → linear), which is standard in MHA. This should be included in implementation.

---

#### B2.2 Spatial routing (Lines 284-287)

**Formula:** $\alpha_q = \text{softmax}\left(\frac{W_r z_q \cdot (W_k^r C_t)^\top}{\sqrt{d}}\right), \quad R_q = \text{TopR}(\alpha_q, R)$

**[OK with note]** — The bilinear routing form ($z_q^\top W_r^\top W_k^r C_t$) is more expressive than simple dot-product routing used in MoE (Switch Transformer uses just $W \cdot x$). The extra key projection $W_k^r$ adds ~16K parameters but allows the routing to learn in a different subspace from the content.

$\sqrt{d}$ scaling with $d = 128$ is correct (prevents logit magnitudes from growing with dimension). ✓

Single-head routing is standard for MoE-style token selection. ✓

---

#### B2.3 Nearest anchor (Lines 292-296)

**Formula:** $i_q^{\text{loc}} = \arg\min_{i \in \{1, \ldots, P_c\}} \|\mathbf{c}_i - q^{(3)}\|_2^2$ where $q^{(3)} = (u/s_3, v/s_3)$.

#### Finding F07 [AMBIGUITY] — Symbol overloading between B2.3 and B3.1

Two different quantities use similar notation:
- $\mathbf{c}_i$ (bold, in B2.3): the **spatial position** of grid cell $i$ in level-3 coordinates — a 2D vector
- $c_r$ (non-bold, in B3.1): the **content vector** of anchor $r$ from $C_t$ — a 128-dim vector

And in B3.1:
- $\mathbf{c}_r$ (bold): the spatial position of anchor $r$ in **pixel coordinates** (not level-3!)
- $c_r$ (non-bold): the content vector

So $\mathbf{c}_i$ in B2.3 is in level-3 coordinates, but $\mathbf{c}_r$ in B3.1 is in pixel coordinates. These are the same anchor but in different coordinate systems. An implementer could easily confuse them.

**Fix:** Use distinct symbols. For example:
- $\mathbf{p}_r$ for the spatial position (in pixel coordinates)
- $c_r$ for the content vector
- Define $\mathbf{p}_r^{(\ell)} = \mathbf{p}_r / s_\ell$ for level-$\ell$ coordinates explicitly

---

### B3. Deformable Multiscale Read

#### B3.1 Conditioning (Lines 334-341)

**Formula:**
$$\Delta\mathbf{c}_r = \mathbf{c}_r - q \quad \text{(pixel coordinates)}$$
$$u_r = \text{LN}(W_u [z_q;\; c_r;\; \phi(\Delta\mathbf{c}_r)] + b_u), \quad \text{input dim } 272$$

Input dimension check: $z_q (128) + c_r (128) + \phi(\Delta\mathbf{c}_r) (16) = 272$. ✓

**See Finding F04** about the Fourier encoding of $\Delta\mathbf{c}_r$. The offset is in pixel coordinates and can be very large — the PE frequencies must be appropriate.

**See Finding F07** about the bold/non-bold $c_r$ symbol overloading.

The DCNv2-style content-aware conditioning design is well-motivated. ✓

---

#### B3.2 Offset and weight prediction (Lines 348-361)

**Formula:**
$$\Delta p_{r,h,\ell,m} = \kappa \cdot \tanh(W_{h,\ell,m}^\Delta \, u_r + b_{h,\ell,m}^\Delta)$$
$$\beta_{r,h,\ell,m} = W_{h,\ell,m}^a \, u_r + b_{h,\ell,m}^a$$

Per-(h,ℓ,m) notation is mathematically equivalent to shared linear layers `nn.Linear(d, H*L*M*2)` + reshape. ✓ (verified in Deformable DETR codebase)

#### Finding F08 [BUG] — False attribution of tanh bounding to Deformable DETR

Line 355 claims: *"This matches Deformable DETR's practice where offsets have similar magnitude at each level's native resolution."*

**This is factually incorrect.** Verified against the official Deformable DETR implementation (`MultiScaleDeformableAttentionFP32` in the InternImage/mmcv codebase):

```python
sampling_offsets = self.sampling_offsets(query).view(
    bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
# Raw linear output — NO tanh, NO clamping, NO bounding
```

Deformable DETR uses **unbounded offsets**. The offsets are divided by `spatial_shapes` (level spatial dimensions) to convert to normalized coordinates, but there is no $\tanh$ or $\kappa$ bounding. Offsets are initialized in a circular pattern at increasing distances but are free to grow during training.

**The EventSPD design choice of $\kappa \cdot \tanh(\cdot)$ is valid as a regularization mechanism** (prevents extreme offsets, especially early in training), but it should NOT be attributed to Deformable DETR.

**Fix:**
1. Remove "This matches Deformable DETR's practice" claim
2. Replace with honest justification: "We bound offsets via $\kappa \cdot \tanh(\cdot)$ to prevent extreme sampling during early training. Deformable DETR leaves offsets unbounded (relying on initialization); our bounding is a conservative choice. If ablations show tanh saturation limits offset diversity, remove the bound."
3. Note that tanh gradient saturates at extremes (gradient < 0.07 for |x| > 2), which could limit the model's ability to reach boundary offsets. Consider a softer bound like $\kappa \cdot \text{softsign}(\cdot)$ or $\kappa \cdot x / \sqrt{1 + x^2}$ as alternatives.

---

#### B3.3 Sampling (Lines 364-371)

**Formula:**
$$p_{\text{sample}} = \mathbf{c}_r^{(\ell)} + \Delta p_{r,h,\ell,m}$$
$$f_{r,h,\ell,m} = \text{GridSample}_{\text{reflect}}(F_t^{(\ell)}, \text{Normalize}(p_{\text{sample}}))$$

#### Finding F09 [BUG] — False claim about reflective padding

Line 371 claims: *"Reflective padding avoids border-collapse artifacts (proven better than clamp in Deformable DETR implementations)."*

**This is factually incorrect.** Verified exhaustively across the InternImage/Deformable DETR codebase:

```python
# From multi_scale_deformable_attn_pytorch (Python fallback):
F.grid_sample(value_l_, sampling_grid_l_, mode='bilinear', padding_mode='zeros', align_corners=False)

# From DCNv3:
F.grid_sample(input_, sampling_grid_, mode='bilinear', padding_mode='zeros', align_corners=False)
```

**Every single instance** of `padding_mode` across the entire codebase (Deformable DETR, DCNv3, BEVFormer) uses `'zeros'`. There is **zero usage** of `'reflect'` anywhere. The CUDA kernel also implements zero-padding for out-of-bound samples.

**Fix:**
1. Change `GridSample_reflect` to `GridSample_zeros`
2. Remove the false claim entirely
3. Replace with: "Zero padding for out-of-bound samples (standard in Deformable DETR). Alternatively, `'border'` padding can be tested if border artifacts are observed."

#### Finding F10 [AMBIGUITY] — c_r^(ℓ) undefined

The formula uses $\mathbf{c}_r^{(\ell)}$ (the anchor center in level-$\ell$ coordinates) but this is never defined. The implicit conversion is $\mathbf{c}_r^{(\ell)} = \mathbf{c}_r / s_\ell$ (pixel coordinates divided by stride).

**Fix:** Define explicitly: "where $\mathbf{c}_r^{(\ell)} = \mathbf{c}_r / s_\ell$ maps the anchor's pixel-coordinate position to level-$\ell$'s native coordinate system."

---

#### B3.4 Per-anchor aggregation (Lines 373-385)

**Formula:**
$$a_{r,h,\ell,m} = \frac{\exp(\beta_{r,h,\ell,m})}{\sum_{h',\ell',m'} \exp(\beta_{r,h',\ell',m'})}, \quad h_r = \sum_{h,\ell,m} a_{r,h,\ell,m} \, f_{r,h,\ell,m}$$

#### Finding F11 [CONCERN] — No true multi-head structure

In standard Deformable DETR (verified in codebase):
1. **Value projection splits by head:** Each head operates on $d_{\text{head}} = d / H$ dimensional features
2. **Per-head aggregation:** Weighted sum within each head's subspace
3. **Concat + output_proj:** Heads are concatenated to $d$ dimensions, then projected via $W_O$

In EventSPD's formula:
1. $f_{r,h,\ell,m} \in \mathbb{R}^d$ — all heads sample full $d$-dimensional features (no head partitioning)
2. Single weighted sum over ALL (h, ℓ, m) — no per-head independence
3. No concat, no $W_O$ output projection

**This means "heads" in EventSPD are not true attention heads.** They are simply additional sampling points. The multi-head diversity that allows different heads to learn complementary patterns (e.g., one head for fine boundaries, another for broad structure) is lost.

**Evidence:** Deformable DETR (Zhu et al., 2020) Eq. 2 explicitly defines multi-scale deformable attention with per-head value projections $W'_m$ and concatenation.

**Recommendation:** Two options:
1. **Follow standard multi-head deformable attention:** Split features into $H$ heads of $d/H = 32$ dims, per-head softmax over $(ℓ, m)$, concat, output_proj. This adds a value projection (128×128) and output projection (128×128) — ~33K more parameters.
2. **Keep current simplified version** but clearly document it as a deliberate simplification: "Unlike standard multi-head deformable attention, we use a single weighted average over all samples (heads serve as additional sampling points, not independent subspaces). The downstream fusion transformer provides multi-head processing."

**If option 2 is chosen:** Note that this means each anchor's evidence $h_r$ is a **convex combination of raw features** (all weights sum to 1, all features are $d$-dimensional). This limits the representational capacity compared to proper multi-head attention.

#### Finding F12 [CONCERN] — Softmax scope differs from Deformable DETR

Related to F11: The softmax normalizes over all $(h, \ell, m)$ jointly within each anchor:
$$a_{r,h,\ell,m} = \text{softmax}_{(h,\ell,m)}(\beta_{r,h,\ell,m})$$

In Deformable DETR, softmax is **per-head**:
$$a_{h,\ell,m} = \text{softmax}_{(\ell,m)}(\beta_{h,\ell,m})$$

Per-head softmax preserves each head's independent weight distribution. Joint softmax allows one dominant head to suppress all others.

**Fix if keeping simplified version:** At minimum, change to per-head softmax: $a_{r,h,\ell,m} = \text{softmax}_{(\ell,m)}(\beta_{r,h,\ell,m})$. This gives each head its own distribution over $(ℓ, m)$ without requiring full multi-head value projection.

---

### B4. Fusion Decoder

#### B4.1 Context token assembly (Lines 400-414)

**Formula:** $T_q = [l_q + e_{\text{loc}}; \; h_{r_1} + e_{\text{near}}; \; h_{r_2} + e_{\text{route}}; \; \ldots; \; h_{r_5} + e_{\text{route}}; \; \bar{c}_q + e_{\text{glob}}]$

#### Finding F13 [CONCERN] — Heterogeneous normalization of T_q components

The 7 tokens in $T_q$ come from different computational paths with potentially different output distributions:

| Token | Source | Final normalization? |
|-------|--------|---------------------|
| $l_q$ | MaxPool(GELU(Linear)) + Linear | **No** — raw linear output |
| $h_{r_1..5}$ | Deformable read aggregation | **No** — weighted average of raw features |
| $\bar{c}_q$ | MHCrossAttn output | **Depends** on whether output_proj includes a normalization |

By contrast, $h_{\text{point}}$ (the query) exits B1.4 with LayerNorm.

When these heterogeneous tokens are assembled into $T_q$ and used as KV in cross-attention, the attention logits ($Q^\top K$) may be dominated by whichever token type has the largest norm. This can create systematic attention bias.

**See Finding F14 for the recommendation.**

#### Finding F14 [SUGGESTION] — Add LayerNorm on KV tokens

**SAM's decoder** (our primary architectural reference) applies separate LayerNorm to both queries AND keys before cross-attention:

```python
# SAM TwoWayAttentionBlock:
q = self.norm2(queries)      # Q normalized
k = self.norm3(keys)         # KV normalized
attn_out = self.cross_attn_token_to_image(q=q+query_pe, k=k+key_pe, v=k)
```

**InternImage's AttentiveBlock** goes further with separate LN for Q, K, and V.

**DETR and Mask2Former** do NOT normalize KV — but their KV comes from an encoder with a final LayerNorm on its output (so the features arrive pre-normalized).

**For EventSPD:** Since $T_q$ tokens come from diverse computational paths (see F13), apply a shared $\text{LN}_{\text{kv}}$ on the assembled $T_q$ before using it as KV:

$$h_{\text{point}} \leftarrow h_{\text{point}} + \text{MHCrossAttn}(Q = \text{LN}_q(h_{\text{point}}), \; KV = \text{LN}_{\text{kv}}(T_q))$$

Since $T_q$ is static (not updated across layers), this LN can be applied once before the 2-layer loop — essentially free.

---

#### B4.2 Transformer layers (Lines 419-424)

**Formula:**
$$h_{\text{point}} \leftarrow h_{\text{point}} + \text{MHCrossAttn}(Q = \text{LN}(h_{\text{point}}), \; KV = T_q)$$
$$h_{\text{point}} \leftarrow h_{\text{point}} + \text{FFN}(\text{LN}(h_{\text{point}}))$$

**[OK]** — Standard Pre-LN transformer. Verified against SAM, DETR, Mask2Former. Both attention and FFN residuals preserved. ✓

**Static T_q across layers:** Correct and standard. 5 out of 6 major architectures (DETR, Deformable DETR, Mask2Former, original Transformer, Perceiver IO) use static KV in the decoder. Only SAM updates KV (via bidirectional attention on 4K image tokens — not applicable to 7 curated tokens). ✓

**7 context tokens:** Sufficient. Each is an information-dense summary (not a raw spatial feature). Precedent: Slot Attention (Locatello et al., 2020) uses 7-11 slots; Set Transformer (Lee et al., 2019) uses $m = 16$ inducing points. The cross-attention's job is to **fuse** pre-processed evidence, not discover spatial information. ✓

**4 heads × 7 KV tokens:** Each head computes a $1 \times 7$ attention distribution. This is very small but sufficient for the fusion task — each head can specialize in weighting different token types. ✓

---

### B5. Depth Prediction

#### B5.1 Relative disparity code (Lines 459-463)

**Formula:** $r_q = W_{r2} \cdot \text{GELU}(W_{r1} \, h_{\text{fuse}} + b_{r1}) + b_{r2}$

#### Finding F15 [AMBIGUITY] — Hidden dimension unspecified

The plan says "A 2-layer MLP with GELU activation. Output: scalar $r_q \in \mathbb{R}$" but does not specify the hidden dimension.

From the parameter table: "B5: Depth head ~17K." With hidden=128: $128\times128 + 128 + 128\times1 + 1 = 16{,}641 \approx 17\text{K}$. So hidden=128 is implied.

**Comparison with published methods:**
- DAv2 (vitb): 128→64→32→1 (gradual compression)
- ZoeDepth: 128→128→1 (same hidden as input)
- NeRF-family: $d \to d/2 \to 1$ (halved hidden)

**Recommendation:** Specify hidden=128 explicitly (matches the ~17K parameter budget). Alternatively, hidden=64 (d/2) would give ~8.3K — more efficient but potentially less capable. The current implied hidden=128 is fine.

#### Finding F16 [AMBIGUITY] — "Relative disparity" terminology

The term "relative disparity" is non-standard and potentially misleading. "Disparity" specifically refers to stereo geometry (pixel displacement between left/right views), which is irrelevant for a monocular decoder.

**Standard terminology:**
- MiDaS: "inverse depth" or "relative depth"
- Monodepth2: "disparity" (justified because it IS a stereo method)
- DAv2: "relative depth"

**Fix:** Rename "Relative disparity code" to "Relative depth code" throughout. The variable name $r_q$ can stay.

---

#### B5.2 Center auxiliary (Lines 466-472)

**Formula:** $r_q^{\text{ctr}} = W_{\text{ctr},2} \cdot \text{GELU}(W_{\text{ctr},1} \, h_{\text{point}}^{(0)} + b_{\text{ctr},1}) + b_{\text{ctr},2}$

128→64→1 with GELU. Parameters: $128\times64 + 64 + 64\times1 + 1 \approx 8.3\text{K}$. ✓

#### Finding F17 [AMBIGUITY] — Does the auxiliary prediction get calibrated?

$r_q^{\text{ctr}}$ is a raw scalar from a 2-layer MLP on the pre-fusion center token. The loss is:
$$L_{\text{ctr}} = \text{Huber}(r_q^{\text{ctr}} - \rho^*(q))$$

**Question:** Should this be $\text{Huber}(s_t \cdot r_q^{\text{ctr}} + b_t - \rho^*(q))$?

If $r_q^{\text{ctr}}$ is uncalibrated but $\rho^*(q) = 1/d^*(q)$ is the absolute ground truth inverse depth, then the auxiliary MLP must simultaneously learn:
1. The depth estimation function (how to map features to depth)
2. The implicit calibration (what scale/shift to use)

This is a harder optimization problem. The main depth head benefits from explicit $(s_t, b_t)$ calibration — the auxiliary should too.

**In PSPNet/DeepLabV3:** Auxiliary heads predict the same output type (class probabilities) as the main head and are supervised with the same labels. They don't need separate calibration because the output space is already calibrated (one-hot labels).

**For depth:** The output space IS uncalibrated (raw codes need scale/shift). Without calibration, the auxiliary loss generates noisy gradients because the MLP must fight against the absolute scale mismatch on top of learning depth estimation.

**Recommendation:** Apply calibration to the auxiliary: $L_{\text{ctr}} = \text{Huber}(s_t \cdot r_q^{\text{ctr}} + b_t - \rho^*(q))$. This lets the auxiliary focus on depth structure, with $(s_t, b_t)$ handling the absolute scale. The gradient flows to both the auxiliary MLP and the calibration heads, which is desirable (it adds another supervision signal for $s_t, b_t$).

---

#### B5.3 Calibration and conversion (Lines 475-484)

**Formula:**
$$\rho_q = s_t \cdot r_q + b_t$$
$$\hat{d}_q = \frac{1}{\text{softplus}(\rho_q) + \varepsilon}$$

**[OK with notes]** — The design is sound and well-justified.

- Softplus ensures positivity: $\text{softplus}(x) = \ln(1 + e^x) > 0$. Smoother than ReLU. ✓
- $\varepsilon = 10^{-6}$ prevents division by zero. ✓
- Scalar $(s_t, b_t)$ calibration on scalar $r_q$ follows MiDaS/Monodepth2 pattern. ✓

**Gradient behavior:**
- For large positive $\rho_q$ (near objects): softplus $\approx \rho_q$, gradient is clean
- For large negative $\rho_q$ (far objects): softplus $\to 0$, depth $\to 1/\varepsilon$, gradient of sigmoid $\to 0$
- Near $\rho_q = 0$: softplus = 0.693, depth = 1.44, gradient = 0.5 — numerically fine

The gradient vanishing for very far objects (large negative $\rho_q$) is inherent to inverse depth representations, not specific to softplus. SiLog loss in log-space naturally mitigates this by compressing the depth range.

**Comparison:** DAv2 metric uses $\hat{d} = \text{sigmoid}(\cdot) \cdot d_{\max}$ — no inversion, bounded output. Monodepth2 uses $\hat{d} = 1/(a \cdot \text{sigmoid}(\cdot) + b)$ — bounded inverse depth. EventSPD's softplus approach is unbounded (no maximum depth), which is more flexible for outdoor scenes with large depth ranges. ✓

---

## Section 5: Training

### 5.1 Loss Functions (Lines 554-586)

#### Finding F18 [AMBIGUITY] — L_point: what is ρ̂(q)?

**Formula:** $L_{\text{point}} = \frac{1}{|Q_v|} \sum_{q \in Q_v} \text{Huber}(\hat{\rho}(q) - \rho^*(q))$

The plan says "Pointwise Huber loss on inverse depth" but $\hat{\rho}(q)$ is ambiguous:

**Interpretation A:** $\hat{\rho}(q) = \rho_q = s_t \cdot r_q + b_t$ (pre-softplus). This can be negative while $\rho^* = 1/d^* > 0$.

**Interpretation B:** $\hat{\rho}(q) = \text{softplus}(\rho_q) + \varepsilon$ (post-softplus). Always positive, matches $\rho^*$.

**Which is correct?** Both have trade-offs:
- Pre-softplus: Gradients flow directly to $r_q$ without softplus nonlinearity. Standard in MiDaS-style training (loss on raw output before any activation). But requires $\rho^*$ to be in the same uncalibrated space.
- Post-softplus: Loss on positive values matching positive targets. More natural for Huber loss. But gradients pass through softplus.

**Looking at the pipeline:** $\rho^*(q) = 1/d^*(q)$ is the ground truth inverse depth (positive). If $\hat{\rho} = \rho_q$ (pre-softplus), the Huber loss would push $\rho_q$ toward positive values, which aligns with softplus's expected input range. This is actually reasonable.

**Recommendation:** Specify explicitly. I recommend post-softplus for consistency: $\hat{\rho}(q) = \text{softplus}(\rho_q) + \varepsilon$, matching the domain of $\rho^*(q) = 1/d^*(q)$.

---

#### Finding F19 [SUGGESTION] — L_silog: missing sqrt()

**Formula:** $L_{\text{silog}} = \frac{1}{|Q_v|} \sum_{q} \delta_q^2 - \lambda_{\text{var}} \left(\frac{1}{|Q_v|} \sum_{q} \delta_q\right)^2$

The formula is **mathematically correct** per Eigen et al. (2014). However, the dominant convention in modern depth estimation uses $\sqrt{\cdot}$ around the expression:

| Method | Uses sqrt? |
|--------|-----------|
| F^3 (your codebase, `SiLogLoss`) | **Yes:** `torch.sqrt(mean(d^2) - lambd * mean(d)^2)` |
| BTS (Lee et al., 2019) | **Yes** (+ multiply by 10) |
| AdaBins (Bhat et al., 2021) | **Yes** |
| DepthAnythingV2 | **Yes** |
| Eigen et al. (2014) original | No sqrt for loss differentiation (but SiLog *metric* uses sqrt) |

**Why sqrt matters:**
- Without sqrt: gradient magnitudes scale with loss magnitude (quadratic behavior). Large errors produce disproportionately large gradients.
- With sqrt: gradient magnitudes are more stable (L1-type scaling). More numerically stable near zero.

**Recommendation:** Add sqrt to match F^3 (the codebase you're building on) and the dominant convention:
$$L_{\text{silog}} = \sqrt{\frac{1}{|Q_v|} \sum_{q} \delta_q^2 - \lambda_{\text{var}} \left(\frac{1}{|Q_v|} \sum_{q} \delta_q\right)^2}$$

**Note on SiLog domain:** Applying SiLog on depth $d$ vs inverse depth $1/d$ is **mathematically identical** because $\delta = \log d - \log d^* = -(\log(1/d) - \log(1/d^*))$ and both the squared term and mean-squared term square the sign away. F^3's codebase applies SiLog on disparity (inverse depth), which is equivalent.

**v5 decision: Fixed.** Added sqrt() to v5 SiLog formula, matching F^3/BTS/AdaBins/DAv2 convention.

---

#### Finding F20 [SUGGESTION] — λ_var = 0.85 vs F^3's λ = 0.5

The plan uses $\lambda_{\text{var}} = 0.85$, which comes from BTS's `variance_focus` parameter.

**F^3's codebase** uses $\lambda = 0.5$:
```python
class SiLogLoss(nn.Module):
    def __init__(self, lambd=0.5):  # default 0.5
```

And in F^3's evaluation metric: `silog = sqrt(mean(d^2) - 0.5 * mean(d)^2)`.

**Effect of λ:**
- $\lambda = 0$: Pure MSE in log-space (no scale invariance)
- $\lambda = 0.5$: Eigen's original (moderate scale invariance)
- $\lambda = 0.85$: BTS-style (more aggressive scale invariance)
- $\lambda = 1$: Fully scale-invariant (only structure matters)

Higher $\lambda$ is more scale-invariant, which is defensible when a separate metric loss ($L_{\text{point}}$) handles absolute accuracy. But for consistency with F^3 and easier debugging, starting with $\lambda = 0.5$ is safer.

**Recommendation:** Use $\lambda_{\text{var}} = 0.5$ as default (matching F^3). Add $\lambda_{\text{var}} \in \{0.5, 0.85, 1.0\}$ to ablation plan. The current 0.85 is not wrong but should be justified against the F^3 baseline.

**v5 decision: Fixed.** Changed to λ_var = 0.5 (F^3 default). Ablation with λ_var ∈ {0.5, 0.85, 1.0} included in v5 plan.

---

**L_ctr and total loss** — See Finding F17 for calibration ambiguity. Otherwise correct:
- $L_{\text{ctr}}$: Huber on auxiliary prediction. PSPNet/DeepLabV3 precedent. ✓
- $\lambda_{\text{si}} = 0.5$, $\lambda_{\text{ctr}} = 0.25$: Reasonable (PSPNet uses 0.4 for auxiliary weight). ✓
- "Only 3 losses" — well-justified, minimal complexity. ✓

---

## Cross-Section Consistency Check

### Parameter Table Consistency

| Component | Claimed | Computed | Match? |
|-----------|---------|----------|--------|
| A2 Pyramid | ~17K | 4 × (32×128 + 128) = 16,896 | ✓ |
| A3 Latent bank | ~17K | 128×128 + 128 = 16,512 | ✓ |
| B1 Center MLP | ~86K | 544×128 + 128 + 128×128 + 128 = 86,144 | ✓ |
| B1 Local MLP | ~37K | 144×128 + 128 + 128×16 + 16 + 128×128 + 128 = 37K (approx) | ✓ |
| B3 Conditioning | ~54K | 272×128 + 128 + 128×96 + 96 + 128×48 + 48 ≈ 54K | ✓ |
| B4 Type embeddings | ~0.5K | 4 × 128 = 512 | ✓ |
| B4 Fusion (2 layers) | ~400K | 2 × (128×128×4 + 128 + 128×512 + 512 + 512×128 + 128 + 128) ≈ 400K | ✓ |
| B5 Depth head | ~17K | 128×128 + 128 + 128×1 + 1 = 16,641 | ✓ |
| B5 L_ctr MLP | ~8K | 128×64 + 64 + 64×1 + 1 = 8,257 | ✓ |
| **Total** | **~772K** | | ✓ |

### Feature Lookup Consistency

| Source | Count | Verified? |
|--------|-------|-----------|
| Center features (B1) | 4 | ✓ |
| Local samples (B1) | 32 | ✓ (24 fixed + 8 learned) |
| Deformable reads (B3) | 240 | ✓ (5 anchors × 4 heads × 4 levels × 3 samples) |
| **Total** | **276** | ✓ |

### Dimension Flow Consistency

| Transition | In | Out | Verified? |
|------------|-----|------|-----------|
| $F_t$ → $F_t^{(\ell)}$ | 32 | 128 | ✓ (Proj 1×1) |
| $[f_q^{(1..4)}; \text{pe}_q]$ → $h_{\text{point}}$ | 544 | 128 | ✓ |
| $[f_q^{(1)}; l_q; \text{pe}_q]$ → $z_q$ | 288 | 128 | ✓ |
| $[z_q; c_r; \phi(\Delta\mathbf{c}_r)]$ → $u_r$ | 272 | 128 | ✓ |
| $u_r$ → offsets | 128 | 96 (H×L×M×2) | ✓ |
| $u_r$ → weights | 128 | 48 (H×L×M) | ✓ |
| $h_{\text{fuse}}$ → $r_q$ | 128 | 1 | ✓ |
| $h_{\text{point}}^{(0)}$ → $r_q^{\text{ctr}}$ | 128 | 1 (via 64 hidden) | ✓ |

---

## Priority Action Items

### Must Fix (BUGs)

1. **F08:** Remove false attribution of tanh bounding to Deformable DETR. State it as a custom design choice.
2. **F09:** Change `GridSample_reflect` to `GridSample_zeros`. Remove false claim about reflective padding.

### Should Fix (CONCERNs)

3. **F04:** Normalize B3 offset before Fourier encoding (divide by image diagonal or max offset). Currently the PE produces meaningless values for large pixel offsets.
4. **F11/F12:** Decide on multi-head deformable attention structure. At minimum, change to per-head softmax (F12). Ideally, adopt proper multi-head value projection (F11).
5. **F13/F14:** Add LN_kv on assembled T_q before using as KV in cross-attention.

### Should Clarify (AMBIGUITIEs)

6. **F01:** Update symbol table to match formula (AvgPool, not strided conv).
7. **F03:** Add `Normalize()` to B1.1 bilinear formula; specify `align_corners=False` convention.
8. **F07:** Rename position symbol to $\mathbf{p}_r$ to avoid confusion with content $c_r$.
9. **F10:** Define $\mathbf{c}_r^{(\ell)} = \mathbf{c}_r / s_\ell$ explicitly.
10. **F15:** Specify depth head hidden dim = 128 (or 64 with adjusted param count).
11. **F16:** Rename "relative disparity" to "relative depth code."
12. **F17:** Specify whether $r_q^{\text{ctr}}$ is calibrated by $(s_t, b_t)$ before $L_{\text{ctr}}$. Recommend: yes.
13. **F18:** Specify $\hat{\rho}(q) = \text{softplus}(\rho_q) + \varepsilon$ in $L_{\text{point}}$.

### Should Consider (SUGGESTIONs)

14. **F05:** Ablate wider center MLP hidden dim (256 or 512 → 128).
15. **F19:** Add sqrt() to SiLog loss to match F^3/BTS/AdaBins convention. **v5: done.**
16. **F20:** Start with λ_var = 0.5 (F^3 default), ablate to 0.85. **v5: done.** Ablation with {0.5, 0.85, 1.0} included.

---

---

## Deep-Dive Re-Investigation (2026-02-15, Round 2)

This section provides deeper analysis of every BUG and CONCERN finding, incorporating extensive literature research, codebase verification, and architecture-wide reasoning. Each finding includes a **confidence level** for the recommendation.

Confidence scale:
- **HIGH (95%)**: Strong published evidence, verified in code, clear consensus
- **MEDIUM-HIGH (80%)**: Good evidence, minor ambiguity in applicability to our context
- **MEDIUM (60%)**: Mixed evidence or context-dependent; ablation needed
- **LOW-MEDIUM (40%)**: Theoretical concern without strong empirical backing

---

### F08 Deep-Dive: Offset Bounding with tanh·κ

**Original finding:** BUG — false claim that tanh bounding "matches Deformable DETR's practice."

#### New evidence

The tanh·κ bounding pattern has **direct published precedent** in **DAT (Deformable Attention Transformer, CVPR 2022 Best Paper Finalist)**:

```python
# DAT (LeapLabTHU/DAT/dat_blocks.py):
offset = offset.tanh().mul(offset_range).mul(self.offset_range_factor)
```

DAT uses `offset_range_factor` values of [1, 2, 3, 4] per stage. The ablation showed the model is "robust to this hyper-parameter" across s=1 to s=10.

**DAT++ (2023, journal extension)** then **removed** tanh bounding entirely, replacing it with simple coordinate clipping to [-1, +1]. The accuracy drop was only **-0.3%** on ImageNet. The authors stated: "Another dispensable design is the restriction of the range of sampling offsets."

**Other methods using offset constraints:**
- **LDConv (2025)**: tanh-based "adaptive limiting units" — motivation: "unconstrained offsets may lead to instability"
- **Offset-Adjusted Mask2Former (2025)**: Three differentiable offset-adjustment strategies for small-organ segmentation, achieving +13.6 Dice from constrained offsets

**Methods with unbounded offsets:**
Deformable DETR, DCNv2, DCNv3, DCNv4, DAB-DETR, Conditional DETR, DN-DETR, DINO-DETR, Mask2Former — all unbounded. Deformable DETR compensates via: (1) zero-initialized weights with structured bias, (2) **0.1× learning rate** on offset heads, (3) normalization by spatial dimensions.

**Why EventSPD needs bounding more than Deformable DETR:** Deformable DETR's offsets come from a zero-initialized linear layer with 0.1× LR — offsets start small and grow slowly. EventSPD's offsets come from an MLP conditioned on u_r (a rich 128-dim vector) — without bounding, offsets could immediately explore extreme values. Evidence from DCNv2 issue #69: unbounded offsets without sufficient regularization can reach mean ~100 with 6-8% accuracy degradation.

#### κ=8 analysis

At level 4 (stride 8): κ=8 → ±64 original pixels → ±5% of image width. The 16×16 latent grid has cells of ~80×45 pixels. κ=8 at level 4 exceeds the cell size, so offsets can reach into neighboring cells. DAT's default s=2 on 14×14 maps gives ~14% reach — EventSPD's 5% is more conservative but sufficient given anchors are already routed to relevant regions.

#### tanh gradient saturation

At pre-activation 2.0: gradient = 0.57 (7% of max). At 3.0: gradient = 0.08 (1%). The multi-scale design mitigates this: if a level-1 offset hits the κ=8 wall, the same spatial location is reachable by a smaller offset at level 2 (needing only κ=4). **ODConv (2022)** explicitly identified this: "when only tanh is added, offset loss is constrained to [-1,1], indicating not all offsets are trained effectively." Their solution: decouple direction (tanh) from magnitude (ReLU).

#### Architecture-wide perspective

The tanh bounding interacts with F04 (Fourier PE): if φ(Δc_r) is broken, the offset head lacks spatial awareness and relies more on content-based signals from z_q and c_r. In this regime, bounded offsets provide a safety net preventing random long-range sampling. Fixing F04 (normalizing PE) would make the offset head spatially aware, reducing the need for strict bounding. The two fixes reinforce each other.

#### Revised recommendation — Confidence: HIGH (95%)

1. **Fix the false attribution** — this is factually wrong and must be corrected. Replace with: *"We bound offsets via κ·tanh(·), following DAT (Xia et al., CVPR 2022). Deformable DETR uses unbounded offsets with 0.1× LR; DAT++ showed the bound is dispensable at -0.3% accuracy cost."*
2. **Keep tanh·κ with κ=8** as default — well-supported by DAT, appropriate for EventSPD's MLP-based offset prediction.
3. **Add to ablation plan**: tanh·κ vs softsign·κ vs DAT++-style clamp. Monitor pre-tanh histogram during training; if >20% of activations exceed |3|, switch to softsign.

**Severity re-assessment: BUG (false attribution) → AMBIGUITY (design is valid, just wrongly cited). The tanh·κ design itself is sound.**

---

### F09 Deep-Dive: GridSample Padding Mode

**Original finding:** BUG — false claim that reflect padding is "proven better in Deformable DETR."

#### Exhaustive search results

| Method | padding_mode | Source |
|--------|-------------|--------|
| Deformable DETR | `'zeros'` | Official code + HuggingFace impl |
| DCNv2 | `zeros` (1px extension for gradients) | Official PyTorch impl |
| DCNv3/InternImage | `'zeros'` | OpenGVLab CUDA kernel |
| DCNv4 | `'zeros'` | Official code |
| DAT/DAT++ | `'zeros'` (PyTorch default) | LeapLabTHU code |
| Mask2Former | `'zeros'` (uses Def DETR module) | Facebook Research code |
| DINO-DETR | `'zeros'` | Same MSDeformAttn module |

**Zero published work uses `'reflect'` for deformable feature sampling.** The search was exhaustive across all major deformable attention/convolution codebases.

#### Semantic analysis of padding modes

- **`zeros`**: Out-of-bound → 0. Creates gradient signal pushing offsets inward. Small discontinuity at boundary. Standard practice.
- **`border`**: Out-of-bound → nearest valid value. No discontinuity but **no gradient** for out-of-bound movement — model cannot learn to pull offsets back. DCNv2 documentation warns about this.
- **`reflect`**: Out-of-bound → reflected. Provides gradient but semantically misleading — reflected features are spatially unrelated to the sampling target.

#### Does it matter with tanh bounding?

With κ=8 and tanh, the maximum offset is ±8 pixels per level. Out-of-bound occurs only when the anchor center c_r^(ℓ) is within 8 pixels of the feature map edge:
- Level 1 (1280×720): boundary zone = 8/1280 = 0.6% per side. ~2.5% of area.
- Level 4 (160×90): boundary zone = 8/160 = 5% per side. ~19% of area.

So at level 4, out-of-bound samples are non-trivial (~19% of anchor positions could produce some OOB samples). But with 48 samples per anchor, the attention weights naturally down-weight anomalous samples.

#### Architecture-wide perspective

This interacts with F08 (tanh bounding): if we remove tanh bounding (per DAT++), more samples could go out-of-bounds, making the padding mode choice more consequential. Keeping tanh bounding reduces the practical impact of padding mode.

#### Revised recommendation — Confidence: HIGH (95%)

1. **Change `GridSample_reflect` to `GridSample_zeros`** — universal standard, no exceptions found.
2. **Remove the false claim entirely** — no published evidence supports it.
3. **Replacement text**: *"Zero padding for out-of-bound samples (standard across all deformable attention implementations: Deformable DETR, DCNv3, DCNv4, Mask2Former). With tanh-bounded offsets, out-of-bound samples are rare except near feature map boundaries at the coarsest level."*

**Severity confirmed: BUG. The false factual claim must be removed.**

---

### F04 Deep-Dive: Fourier PE Frequency Mismatch

**Original finding:** CONCERN — B3 claims "same Fourier encoding" as B1.2 but input ranges differ by 200×.

#### How major methods handle spatial encoding in offset prediction

| Method | Offset PE? | Coordinate space | How position enters |
|--------|-----------|-----------------|---------------------|
| Deformable DETR | **None** | Normalized [0,1] | Reference point + scale-level embedding |
| DAB-DETR | Sinusoidal T=20 | **Normalized [0,1]** | PE(x,y,w,h) through MLP |
| Conditional DETR | Sinusoidal | **Sigmoid→[0,1]** | Conditional spatial query |
| DCNv2/v3/v4 | **None** | Pixel-relative | Content at reference location (via conv) |
| NeRF | Fourier | **Normalized [-1,1]** | Standard positional encoding |
| Tancik et al. | Fourier | **Normalized [0,1]** | Random Fourier features |

**Critical finding: NO method in the DETR family applies Fourier/sinusoidal encoding to raw pixel coordinates. Every method that uses PE first normalizes to [0,1] or [-1,1].**

#### Detailed frequency analysis

**B1.2 (local, |δ|≤6, raw pixel offsets):**
With σ_l = 2^l (l=0..3) on raw pixel offsets: at max δ=6, highest frequency argument = 2π·8·6 = 301.6 rad ≈ 48 full cycles. The Nyquist resolution is 1/16 = 0.0625 pixels — far finer than needed. **This works well because the input range is small.**

**B3 (global, |Δc_r| up to 1280, raw pixel offsets):**
At Δc_r=1280: sin(2π·8·1280) = sin(64,339). **Completely meaningless.** Even the lowest frequency: sin(2π·1·1280) = sin(8,042) — still meaningless oscillation.

**B3 with normalization (Δc_r/[W,H] giving range [-1,1]):**
At max offset 1.0: sin(2π·8·1.0) = sin(50.3) ≈ 8 full cycles — meaningful. But with 4 frequencies, the finest period is 1/8 = 160 pixels. Two anchors separated by <80 pixels get ambiguous encodings. With **8 frequencies** (matching pe_q), finest period = 1/256 ≈ 5 pixels — resolves any anchor difference.

**B1.2 with normalization would BREAK:** If B1.2 normalizes its δ ∈ [-6,6] by image dimensions, max normalized offset = 6/1280 = 0.005. Highest frequency argument = 2π·8·0.005 = 0.25 rad. All outputs are near-zero/near-linear. The encoding becomes useless.

**This proves B1.2 and B3 CANNOT share the same encoding scheme.** They are in fundamentally different regimes.

#### The "MLP learns to ignore it" argument

The conditioning vector u_r = LN(W_u [z_q(128); c_r(128); φ(Δc_r)(16)]) has φ as only 6% of input. If φ is broken, the MLP would learn near-zero weights for those 16 dims, relying on z_q + c_r alone.

**But this is sub-optimal, not benign.** Without spatial encoding:
- z_q is shared across all 5 anchors — provides no anchor-specific information
- c_r (anchor content, 128-dim) differentiates anchors by content but NOT geometry
- Two anchors at opposite sides of the image with similar content (e.g., two wall segments) would get identical offsets — the model cannot learn spatially appropriate sampling patterns

**This is NOT like Deformable DETR**, where the query implicitly carries position through iterative decoder refinement. EventSPD's z_q is constructed once and never updated — it must carry sufficient spatial info from the start.

#### Architecture-wide perspective

F04 compounds with F06 (routing token). If z_q lacks multi-scale features AND φ(Δc_r) is broken in B3, the deformable read has extremely limited spatial awareness. The offset head can only predict content-based offsets (from c_r), not geometry-aware ones. This is like running DCNv2 without the convolutional neighborhood structure — possible but degraded.

Fixing F04 is the highest-impact single change because it restores spatial awareness to the deformable offset prediction, which is the core novel mechanism of the architecture.

#### Revised recommendation — Confidence: HIGH (95%)

**Use two separate Fourier encodings:**

1. **B1.2 (local): Keep as-is.** 4 frequencies on raw pixel offsets in [-6, 6]. Well-matched and working.

2. **B3 (deformable): Normalize offsets, use pe_q-style 8-frequency encoding:**
$$\phi_{\text{B3}}(\Delta\mathbf{c}_r) = [\sin(2\pi \sigma_l \Delta x/W); \cos(2\pi \sigma_l \Delta x/W); \sin(2\pi \sigma_l \Delta y/H); \cos(2\pi \sigma_l \Delta y/H)]_{l=0}^{7}$$
giving **32 dims** (8 freq × 2 trig × 2 spatial dims).

3. **Update B3 conditioning input dim:** 128 + 128 + 32 = **288** (from 272). Parameter cost: +2,048 for W_u (0.3% of total).

4. **Update the text**: Remove "same Fourier encoding." State: *"B3 uses a separate 8-frequency Fourier encoding on normalized offsets [Δx/W, Δy/H], matching pe_q's design. B1.2's 4-frequency encoding on raw pixel offsets is appropriate for its smaller range (|δ|≤6)."*

**Severity upgrade: CONCERN → MUST FIX. The current design produces meaningless spatial encoding for deformable conditioning, crippling the core novel mechanism.**

---

### F11/F12 Deep-Dive: Multi-Head Structure and Softmax Scope

**Original finding:** CONCERN — no true multi-head structure; joint softmax instead of per-head.

#### Published deformable attention structures

| Method | Head structure | Value proj? | Softmax scope | Output |
|--------|--------------|-------------|---------------|--------|
| Deformable DETR | d/H per head | Yes (W'_m) | Per-head over (ℓ,m) | Concat + W_O |
| DCNv3 | channels/G per group | Within-group | Per-group over K | Concat |
| DCNv4 | channels/G per group | Within-group | **No softmax** (unbounded) | Concat |
| DAT | Groups share offsets | Post-sample MHA | Standard MHA softmax | Standard MHA |
| EventSPD | Full d per "head" | None | Joint over (h,ℓ,m) | Weighted avg |

**No published method uses EventSPD's exact simplification.** All published methods either partition features per head/group or apply separate normalization per head/group.

**DCNv4's softmax removal** is interesting but not applicable: DCNv4 argues softmax limits expressiveness in *convolution* operators where each location has a dedicated aggregation window. In attention-like mechanisms (which EventSPD's deformable read is), normalization prevents magnitude explosion.

#### What "heads as extra samples" means in practice

With EventSPD's joint softmax, the output h_r is a **convex combination** of all 48 sampled features (weights sum to 1, all features are 128-dim). This means:
- h_r lies within the convex hull of the 48 sampled features
- The model cannot produce a representation outside this hull
- If all 48 features are similar (large uniform region), h_r ≈ any individual feature — no additional expressiveness

With proper multi-head structure, each head produces a 32-dim vector from its own aggregation. The concat + W_O projection can produce ANY 128-dim output, including ones outside the convex hull of sampled features. This is strictly more expressive.

#### The compensation argument

EventSPD's deformable read output h_r goes into a 2-layer Pre-LN cross-attention transformer with its own 4-head attention. Does the downstream transformer compensate for the lack of multi-head diversity in B3?

**Partially, but not fully:**
- B4's cross-attention has KV = [l_q; h_r1..h_r5; c̄_q]. It attends to 7 tokens, learning which to weight.
- But each h_r token is already a convex combination — it has lost the diversity that multi-head would provide.
- Two different anchors with identical surrounding features would produce identical h_r values, even if multi-head Deformable DETR would have captured complementary patterns.

The analogy: Deformable DETR's encoder (multi-head deformable attention) feeds into the decoder (multi-head cross-attention). These are never considered redundant — they serve orthogonal purposes (spatial aggregation vs query-key matching). B3 and B4 would similarly be complementary.

#### Cost analysis

Adding proper multi-head structure:
- Value projection: W_V: 128→128 (4 heads × 32 dims). **Shared across all anchors.** ~16.5K params.
- Output projection: W_O: 128→128. Shared. ~16.5K params.
- **Total: ~33K params (4.3% of 772K).**

Per-head softmax (F12 fix alone, no value proj): **Zero additional parameters.** Just reshape the softmax from (H·L·M) to (H, L·M) and normalize per head.

#### Architecture-wide perspective

The F11/F12 decision interacts with F05 (center MLP bottleneck). If h_r tokens are merely convex combinations (limited expressiveness), the fusion transformer must work harder to extract useful information. A more expressive h_r (from proper multi-head) would make the fusion more effective and partially relieve pressure on the center token to carry all the discriminative signal.

#### Revised recommendation — Confidence: MEDIUM-HIGH (80%)

**Two-tier fix:**

**Minimum (per-head softmax) — Confidence: HIGH (90%):**
Change the aggregation to per-head softmax:
$$a_{r,h,\ell,m} = \text{softmax}_{(\ell,m)}(\beta_{r,h,\ell,m})$$
$$h_r = \frac{1}{H}\sum_h \sum_{\ell,m} a_{r,h,\ell,m} f_{r,h,\ell,m}$$

This is zero-cost, follows universal practice, and preserves per-head independence. The averaging over heads (instead of concat+W_O) keeps the simplified structure while allowing each head its own distribution. **Every published method uses per-head or per-group normalization.** Joint softmax has zero precedent.

**Ideal (full multi-head) — Confidence: MEDIUM-HIGH (80%):**
Add W_V (128→128, head-partitioned) and W_O (128→128). Per-head softmax over (ℓ,m). Standard Deformable DETR pattern. Cost: +33K (4.3%). Benefit: h_r becomes more expressive (no longer limited to convex hull). Add to ablation: simplified per-head softmax vs full multi-head.

**The per-head softmax fix should be applied unconditionally. The full multi-head is recommended but can be validated via ablation.**

---

### F13/F14 Deep-Dive: Heterogeneous KV Token Normalization

**Original finding:** CONCERN — T_q tokens from different computational paths have different scales.

#### How major architectures handle this

| Architecture | KV source | KV normalized? | How? |
|-------------|-----------|---------------|------|
| SAM | Image embeddings | **Yes** | Separate LN on queries (norm2) and keys (norm3) |
| DETR | Encoder output | **Implicitly** | Encoder has final LN; features arrive pre-normalized |
| Mask2Former | Pixel decoder output | **Implicitly** | Pixel decoder FPN has BN/GN on outputs |
| Perceiver IO | Raw inputs (multimodal) | **Yes** | Input arrays projected to common dim with LN |
| Flamingo | Visual tokens | **Yes** | Perceiver resampler with LN on visual input |
| Standard Pre-LN | Same-source | **Yes** (for Q) | Q = LN(x); KV = raw tokens from same layer |

**Key pattern:** When KV comes from a **single homogeneous source** (DETR encoder output, standard transformer hidden states), implicit normalization from the source's final LN is sufficient. When KV comes from **heterogeneous sources** (multimodal, different computational paths), explicit normalization is standard.

EventSPD's T_q has 3 distinct source types with no shared normalization:
- l_q: raw linear output (no LN)
- h_r1..5: weighted average of pyramid features (no LN)
- c̄_q: MHA output (typically has output_proj but no LN)

This is the heterogeneous case — more similar to Perceiver IO's multimodal input than to DETR's homogeneous encoder output.

#### Does scale mismatch actually hurt with 7 static tokens?

In QKV attention: logit = Q·K^T / √d_head. If K vectors have heterogeneous norms, their dot products with Q will have proportionally different magnitudes. After softmax, higher-norm tokens get disproportionate attention weight — a **systematic bias** unrelated to content relevance.

**Counter-argument:** The model could learn W_K that compensates for scale differences. With only 7 tokens and 4 types, this is feasible — 4 implicit scale factors baked into W_K rows. However, this "wastes" W_K capacity on normalization rather than semantic transformation.

**The type embeddings don't fix this.** Type embeddings are additive (e_loc + l_q). If ||l_q|| = 50 and ||h_r|| = 5, adding e_loc ∈ R^128 can shift but not rescale. The 10× norm difference persists.

#### Shared LN_kv vs per-type LN

- **Shared LN_kv on assembled T_q** (7×128): Normalizes across all 7 tokens together. Simple. Equalizes scales. But: if one token type genuinely has informative magnitude (e.g., high-magnitude h_r = high-confidence region), shared LN destroys this signal.
- **Per-type LN** (separate LN for l_q, for h_r group, for c̄_q): Preserves within-type statistics, normalizes across types. More parameters (~1.5K) but still trivial. More principled.
- **QK-norm** (normalize Q and K independently before dot product): Used in ViT-22B, Scaling ViTs. Handles scale mismatch without modifying token values. But adds a normalization inside attention, which is non-standard for cross-attention decoders.

#### Architecture-wide perspective

F13/F14 interacts with F11/F12: if we add proper multi-head deformable attention (F11 fix), each h_r goes through W_O (output projection), which creates a more normalized output. This reduces (but doesn't eliminate) the heterogeneity of T_q.

F13/F14 also interacts with F02 (latent bank projection): if C_t has proper normalization from the latent bank projection, then c̄_q (which is cross-attention output from C_t) will have more controlled magnitude.

#### Revised recommendation — Confidence: MEDIUM-HIGH (75%)

1. **Apply shared LN_kv on assembled T_q before use as KV** — simple, cheap (256 params), SAM-consistent. Since T_q is static across both transformer layers, this LN is applied once.

2. **Update B4 formula to:**
$$h_{\text{point}} \leftarrow h_{\text{point}} + \text{MHCrossAttn}(Q = \text{LN}_q(h_{\text{point}}), \; KV = \text{LN}_{\text{kv}}(T_q))$$

3. **Add to ablation**: no KV LN (current) vs shared LN_kv vs per-type LN. If per-type LN shows gains over shared, the extra 1K params is justified.

**Confidence is MEDIUM-HIGH rather than HIGH because the small KV set (7 tokens) means the model might learn to compensate via W_K. But the fix is so cheap that there's no reason not to include it.**

---

### F02 Deep-Dive: Redundant 128→128 Projection in Latent Bank

**Original finding:** CONCERN — potentially redundant re-projection.

#### Evidence from major architectures

| Architecture | Intermediate projection before decoder? | What it does |
|-------------|---------------------------------------|-------------|
| DETR | **No** | Encoder output → decoder KV directly |
| SAM | **No** | Image embedding + prompt → transformer directly |
| Perceiver IO | **No** | Latent array initialized as learned params, used directly |

**None of the three major references add a same-dimension projection before decoder cross-attention.** The internal W_K, W_V projections handle representation transformation.

#### Why EventSPD's case is different

However, EventSPD's latent bank undergoes **225:1 spatial compression** (AdaptiveAvgPool over 20×11.25 cells). This fundamentally changes feature statistics via the central limit theorem: pre-pooled features have sparse/asymmetric distributions (from ReLU/GELU), but averaging 225 vectors produces near-Gaussian distributions. The downstream W_K, W_V projections were trained assuming a certain input distribution — pooled features lie in a different manifold region.

The projection serves three roles:
1. **Distribution re-centering** after aggressive pooling
2. **Representation space separation** — latent tokens serve dual roles (KV for summary, routing candidates, anchor content for B3), so a dedicated "latent space" separating pyramid space from token space is useful
3. **Gradient pathway buffer** — absorbs task-specific gradients from routing, summary, and B3 conditioning

#### Architecture-wide perspective

If we remove the projection (F02), the latent bank tokens are raw pooled pyramid features. This means c_r in B3 conditioning is also raw pooled features. Combined with F04 (broken Fourier PE), the deformable conditioning would have: z_q (query content) + raw pooled features (anchor content) + broken spatial encoding. The projection provides at least a learned transformation that can adapt the pooled features for their dual roles.

#### Revised recommendation — Confidence: MEDIUM (60%)

**Keep the projection.** The ~17K cost (2.2% of total) is trivial. The distributional-shift argument has theoretical merit but no direct empirical citation. The three-role justification (distribution, separation, gradient) is sound in principle.

**However, reduce priority.** This is the least impactful issue. Add to ablation plan: (a) current Proj 128→128, (b) no projection, (c) just LayerNorm (no linear, cheaper normalization).

**Severity re-assessment: CONCERN → LOW-PRIORITY ABLATION. Not a bug, not a design flaw, just an open question about whether ~17K params are useful.**

---

### F05 Deep-Dive: Aggressive 544→128 Center MLP Bottleneck

**Original finding:** CONCERN — 4.25× compression in one step may lose multi-scale information.

#### Published multi-scale fusion dimensions

| Method | Multi-scale input | Fusion strategy | Compression ratio |
|--------|------------------|----------------|-------------------|
| DPT | 4 levels × 256-dim | Progressive spatial cascade (no concatenation) | N/A (spatial fusion) |
| DAv2 | 4 levels × 256-dim | Same DPT cascade | N/A |
| PointNet++ FP1 | 1280-dim concat | MLP [256, 256] | **5:1** (1280→256) |
| PointNet++ FP2 | 512-dim concat | MLP [128, 128] | **4:1** (512→128) |
| EventSPD | 544-dim concat | MLP [128, 128] | **4.25:1** (544→128) |

**Key finding:** PointNet++ does comparable ratios (5:1 at FP1, 4:1 at FP2) and achieves strong results. But PointNet++ uses **wider intermediate layers** — FP1's 1280→256→256 has a 256-wide hidden (matching the output), not 128. EventSPD's 544→128→128 has hidden = output, which matches PointNet++ FP2 but is narrower than FP1.

**DPT never concatenates** — it fuses spatially through progressive refinement blocks. This is fundamentally different from EventSPD's point-based concatenation. The DPT comparison is not directly applicable.

#### The "query doesn't need full information" argument

This is the strongest counter-argument: h_point is a **query** for cross-attention, not a final prediction. It needs to encode enough "intent" to ask the right questions of the 7 context tokens (which carry the actual evidence). A cross-attention query doesn't need to reconstruct the original signal — it needs to express what information to retrieve.

**Analogy:** Deformable DETR's 256-dim query drives offset prediction across 4 levels × 8 heads × 4 samples = 128 locations. The query doesn't carry all encoder information — it encodes retrieval intent. EventSPD's 128-dim query drives cross-attention over 7 tokens. The information need is comparable.

**However:** After the transformer, h_fuse goes to the depth MLP (128→128→1). At this point, h_fuse IS the final representation — all task-relevant information must be in 128 dims. The transformer can inject information from context tokens, but the initial h_point determines the "starting point" of this 2-layer refinement. A better h_point → better starting point → likely better depth prediction.

#### Architecture-wide perspective

F05 interacts with F11/F12 (multi-head deformable): if h_r tokens are convex combinations (limited expressiveness from joint softmax), the context tokens carry less information, putting MORE burden on h_point as the primary signal carrier. Fixing F11/F12 (adding multi-head) would reduce the burden on h_point, making the 544→128 bottleneck less critical.

F05 also interacts with the FFN expansion: the transformer's FFN goes 128→512→128 (4× expansion). This 512-wide hidden layer can create complex nonlinear transformations, partially compensating for information lost in the center MLP. But the FFN operates on the output of cross-attention (h_point + attention output), not on the raw 544-dim input — it cannot recover information that was never encoded into h_point.

#### Revised recommendation — Confidence: MEDIUM (55%)

**Keep 544→128→128 as default.** The PointNet++ precedent (4:1 ratio at FP2), the "query intent" argument, and the downstream FFN compensation all support the current design. The MobileNetV2 "wider intermediate" principle is valid but h_point's role as a query (not a final representation) weakens its applicability here.

**Add to ablation table:** 544→128→128 (current, ~86K) vs 544→256→128 (~156K, +70K). The wider variant adds 9% to total parameters — meaningful but not prohibitive for accuracy-first. If the wider variant shows >0.5% improvement on any metric, keep it.

**Severity re-assessment: CONCERN → MEDIUM-PRIORITY ABLATION. The current design is defensible; wider hidden is an optimization opportunity, not a fix for a flaw.**

---

### F06 Deep-Dive: Routing Token Uses Only Level-1 Features

**Original finding:** CONCERN — z_q lacks multi-scale information for routing.

#### MoE routing precedent

| Method | Router input | Context? |
|--------|-------------|---------|
| Switch Transformer | Current token hidden state | **Single token only** |
| GShard | Current token embedding | **Single token only** |
| DeepSeek MoE | Token embedding at current layer | **Single token only** |
| Deformable DETR | Decoder query embedding | Iteratively refined across layers |

**MoE routers universally use only the current token representation.** They do NOT aggregate multi-source information. The assumption: the token at the routing layer already encodes sufficient information.

**BUT the critical difference:** In MoE, the token has been through multiple self-attention layers that absorb broad context. In Deformable DETR, the decoder query is iteratively refined through cross-attention with the encoder. In EventSPD, z_q is constructed **once** from raw features — it has had **no prior attention** to absorb multi-scale context.

#### The textureless region analysis

For a flat white wall at the query pixel:
- **f_q^(1)** (128-dim): 63×63 receptive field, but for a uniform wall, all features are similar. Low discriminative power.
- **l_q** (128-dim): MaxPool over 32 nearby points. For a uniform wall, all points look similar → l_q ≈ f_q^(1). Still uninformative.
- **f_q^(3)** (128-dim, NOT currently used): 252×252 effective context. At this scale, the "wall" feature might see boundaries, adjacent objects, ceiling/floor — useful for routing.

**Counter-argument strength:** l_q's MaxPool over 32 samples spanning a ~12×12 pixel neighborhood with 63×63 receptive field per sample collectively covers a union of receptive fields spanning roughly 75-130 pixels. This IS non-trivial context. For truly textureless regions (large flat walls), routing precision matters LEAST — any anchor on the wall gives similar depth evidence. The hard cases (depth discontinuities) are exactly where level-1 features ARE discriminative.

**Counter-counter-argument:** The routing targets (C_t tokens) are pooled FROM F_t^(3). Matching the representation space (using f_q^(3) to compute affinity against C_t from F_t^(3)) could improve routing quality.

#### Architecture-wide perspective

F06 interacts with F04 (Fourier PE): z_q is shared across all 5 anchors in B3 conditioning. If the deformable conditioning has proper spatial encoding (F04 fixed), then even a "weaker" z_q can still produce good offsets because φ(Δc_r) provides geometric differentiation between anchors. But if both z_q is weak (F06) AND φ is broken (F04), the deformable read has very limited discrimination ability.

The interaction chain: F06 (routing quality) → anchor selection → F04 (spatial encoding in B3) → offset quality → h_r quality → fusion quality → depth quality. Each link weakens the next. Fixing F04 is more impactful because it affects all 5 anchors' offset predictions, while F06 only affects which anchors are selected.

#### Revised recommendation — Confidence: MEDIUM (50%)

**Keep level-1-only routing as default.** The MoE precedent, the l_q local context argument, and the "hard cases are discriminative" argument support the current design.

**Add as ablation option**: z_q = LN(W_z [f_q^(1); f_q^(3); l_q; pe_q]). Input: 416 dims. Cost: +16K params (2%). A cheaper alternative: z_q = LN(W_z [f_q^(1) + f_q^(3); l_q; pe_q]) — element-wise addition preserves 288-dim input (zero extra params) at the cost of destroying scale-specific information.

**Severity re-assessment: CONCERN → LOW-PRIORITY ABLATION. The design is reasonable as default; adding f_q^(3) is a cheap experiment worth running but not a mandatory fix.**

---

## Architecture-Wide Interaction Map

The 8 findings are not independent. They form an interaction chain through EventSPD's information flow:

```
F02 (latent bank proj)
  └──→ C_t quality → routing candidates & anchor content c_r
        ├──→ F06 (routing token) → which anchors selected
        │      └──→ S_q = {anchor set}
        └──→ B3 conditioning
              ├──→ F04 (Fourier PE) → spatial awareness of offset MLP
              │      └──→ offset quality
              ├──→ F08 (tanh bound) → offset range
              │      └──→ sampling reach
              └──→ F09 (padding mode) → boundary handling
                     └──→ f_{r,h,l,m} (sampled features)
                            └──→ F11/F12 (multi-head / softmax)
                                   └──→ h_r quality (per-anchor evidence)
                                          └──→ F13/F14 (KV normalization)
                                                 └──→ T_q quality
                                                        └──→ F05 (center MLP bottleneck)
                                                               └──→ h_point as query
                                                                      └──→ cross-attention fusion
                                                                             └──→ depth prediction
```

### Compounding risk scenario (worst case)

If F04 is not fixed (broken PE) AND F11/F12 are not fixed (no multi-head, joint softmax): the deformable read produces spatially-unaware convex combinations of raw features. Combined with F13/F14 (no KV normalization), the fusion transformer receives poorly-differentiated, scale-mismatched context tokens. The system degrades to "center token + global average" — essentially a Baseline C minimal decoder with extra wasted parameters.

### Minimal fix set for architectural soundness

The three fixes that are architecturally essential (not just improvement opportunities):

1. **F04 (Fourier PE normalization)** — without this, the core novel mechanism (deformable conditioning) is crippled
2. **F08/F09 (false attributions)** — factual errors that must be corrected
3. **F12 (per-head softmax)** — zero-cost fix that follows universal practice

Everything else (F02, F05, F06, F11 full multi-head, F13/F14) can be validated via ablation.

---

## Revised Priority Action Items

### MUST FIX (confidence ≥ 90%)

| # | Finding | Action | Cost | Confidence |
|---|---------|--------|------|------------|
| 1 | **F04** | Normalize B3 offsets [Δx/W, Δy/H], use 8-freq PE (32 dims). Keep B1.2 as-is. | +2K params | **95%** |
| 2 | **F08** | Fix false attribution. Replace "Deformable DETR" with "DAT (CVPR 2022)". Keep tanh·κ. | 0 | **95%** |
| 3 | **F09** | Change reflect→zeros. Remove false claim. | 0 | **95%** |
| 4 | **F12** | Change joint softmax to per-head softmax over (ℓ,m). | 0 | **90%** |

### SHOULD FIX (confidence 70-85%)

| # | Finding | Action | Cost | Confidence |
|---|---------|--------|------|------------|
| 5 | **F11** | Add W_V (128→128) + W_O (128→128) for proper multi-head deformable. | +33K params | **80%** |
| 6 | **F13/F14** | Add shared LN_kv on assembled T_q. | +256 params | **75%** |

### ABLATION ITEMS (confidence 40-65%)

| # | Finding | Ablation | Cost delta | Confidence |
|---|---------|----------|------------|------------|
| 7 | **F05** | 544→128→128 vs 544→256→128 | +70K params | **55%** |
| 8 | **F06** | Level-1-only vs [f_q^(1); f_q^(3)] routing | +16K params | **50%** |
| 9 | **F02** | With vs without latent bank Proj | -17K params | **60%** |

---

## References Cited in This Audit

### Core architecture references
- Zhu et al., "Deformable DETR: Deformable Transformers for End-to-End Object Detection," ICLR 2021 (arxiv 2010.04159) — verified: unbounded offsets, zeros padding, per-head softmax
- Kirillov et al., "Segment Anything (SAM)," ICCV 2023 (arxiv 2304.02643) — separate LN for Q and KV in TwoWayAttentionBlock
- Li et al., "Exploring Plain Vision Transformer Backbones for Object Detection (ViTDet)," ECCV 2022 — independent pyramid principle
- Carion et al., "End-to-End Object Detection with Transformers (DETR)," ECCV 2020 — no intermediate projection before decoder

### Deformable attention variants
- Xia et al., "Vision Transformer with Deformable Attention (DAT)," CVPR 2022 — **tanh·offset_range_factor bounding**, s=1 to 10 all work
- Xia et al., "DAT++: Spatially Dynamic Vision Transformer with Adaptive Tokens," 2023 (arxiv 2309.01430) — **removed tanh bounding, -0.3% accuracy**
- Wang et al., "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions (DCNv3)," CVPR 2023 — per-group softmax, unbounded offsets
- Xiong et al., "Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications (DCNv4)," CVPR 2024 — removed softmax entirely, unbounded weights
- Zhu et al., "Deformable ConvNets v2 (DCNv2)," CVPR 2019 (arxiv 1811.11168) — content-aware offsets, zeros padding
- Liu et al., "DAB-DETR: Dynamic Anchor Boxes Are Better Queries," ICLR 2022 — sinusoidal PE with T=20 on **normalized [0,1] coordinates**
- Meng et al., "Conditional DETR," ICCV 2021 — sigmoid-normalized reference points before PE
- LDConv, "Limit Deformable Convolution," Complex & Intelligent Systems, 2025 — tanh-based offset limiters
- Offset-Adjusted Mask2Former, arxiv 2506.05897, 2025 — differentiable offset constraints

### Positional encoding references
- Tancik et al., "Fourier Features Let Networks Learn High Frequency Functions," NeurIPS 2020 — inputs normalized to [0,1]
- Mildenhall et al., "NeRF: Representing Scenes as Neural Radiance Fields," ECCV 2020 — inputs normalized to [-1,1]
- Barron et al., "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields," ICCV 2021 — IPE for multi-scale
- Li et al., "Learnable Fourier Features for Multi-dimensional Spatial Positional Encoding," NeurIPS 2021

### Depth estimation references
- Eigen et al., "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network," NeurIPS 2014 — SiLog loss
- Ranftl et al., "Vision Transformers for Dense Prediction (DPT)," ICCV 2021 — progressive spatial fusion, never concatenates scales
- Lee et al., "From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation (BTS)," 2019

### Multi-scale fusion references
- Qi et al., "PointNet++: Deep Hierarchical Feature Learning on Point Sets," NeurIPS 2017 — FP layers: 1280→256 (5:1) and 512→128 (4:1) compression
- Qi et al., "PointNet: Deep Learning on Point Sets," CVPR 2017 — MaxPool >> MeanPool
- Sandler et al., "MobileNetV2: Inverted Residuals and Linear Bottlenecks," CVPR 2018 — wider intermediates preserve info through nonlinearities

### Routing / MoE references
- Fedus et al., "Switch Transformers: Scaling to Trillion Parameter Models," JMLR 2022 — router sees single token only
- Locatello et al., "Object-Centric Learning with Slot Attention," NeurIPS 2020
- Lee et al., "Set Transformer: A Framework for Attention-based Set-to-Set Functions," ICML 2019

### Codebase verification
- F^3: `fast-feature-fields-main/src/f3/tasks/depth/utils/losses.py` — SiLogLoss with sqrt(), λ=0.5 default
- DAv2: `fast-feature-fields-main/src/f3/tasks/depth/depth_anything_v2/dpt.py` — ReLU depth head
- Deformable DETR: `fast-feature-fields-main/src/f3/tasks/segmentation/InternImage/...` — zeros padding, unbounded offsets, per-head softmax
- DAT: `github.com/LeapLabTHU/DAT/blob/main/models/dat_blocks.py` — tanh·offset_range verified
- HuggingFace Deformable DETR: `transformers/models/deformable_detr/modeling_deformable_detr.py` — padding_mode="zeros"
