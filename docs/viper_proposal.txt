Sparse Depth Estimation from Event Cameras for Energy-Efficient Robotic Perception

Autonomous robots and edge devices like warehouse drones rely on real-time depth perception to navigate and interact with the world. Modern deep learning has made accurate depth prediction possible, but at a steep energy cost: state-of-the-art neural networks predict a full dense depth map for all pixels in every frame, requiring billions of floating-point operations per inference. This continuous dense computation can dominate the power budget. However, most downstream tasks (grasping an object, avoiding an obstacle) only need depth at a handful of points. Thus, the vast majority of that energy-intensive computation produces values that are immediately discarded.

This project plans to develop a sparse, query-based depth estimation algorithm built on event cameras, which are sensors that report only brightness changes rather than full frames. Instead of producing a complete depth map and discarding most of it, our approach would predict depth only at the specific pixels a downstream task actually needs. To achieve this, we plan to build on an existing event-camera feature encoder to obtain a shared scene representation computed once per time step, then develop a lightweight per-query decoder that extracts depth at each requested pixel by attending to that shared representation, adding only marginal cost per query. The goal is to demonstrate that depth prediction algorithms doesn't necessarily need to rely on dense and energy-unfriendly computations. We want to construct an algorithm that predicts only the points' depth that our downstream tasks require. We plan to evaluate the energy and latency trade-offs, comparing against dense baselines across different query budgets.

By aligning both the sensor and the algorithm with the principle of "compute only what you need," this work aims to directly address the energy bottleneck that limits deploying perception on battery-powered and thermally constrained platforms. We hope to explore whether depth prediction algorithms can be redesigned to consume less energy per inference by avoiding unnecessary computation, potentially helping bring real-time 3D perception closer to the power budgets of drones and mobile robots.
