                                                          Instant Neural Graphics Primitives with a Multiresolution Hash Encoding
                                                          THOMAS MÃœLLER, NVIDIA, Switzerland
                                                          ALEX EVANS, NVIDIA, United Kingdom
                                                          CHRISTOPH SCHIED, NVIDIA, USA
                                                          ALEXANDER KELLER, NVIDIA, Germany
                                                          https://nvlabs.github.io/instant-ngp
                                                                 Trained for 1 second                          15 seconds                         1 second                15 seconds                60 seconds                 reference
                                        Gigapixel image
arXiv:2201.05989v2 [cs.CV] 4 May 2022
                                           SDF
                                           NRC
                                           NeRF




                                                          Fig. 1. We demonstrate instant training of neural graphics primitives on a single GPU for multiple tasks. In Gigapixel image we represent a gigapixel image by
                                                          a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. Neural radiance caching (NRC) [MÃ¼ller et al.
                                                          2021] employs a neural network that is trained in real-time to cache costly lighting calculations. Lastly, NeRF [Mildenhall et al. 2020] uses 2D images and
                                                          their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. In all tasks, our encoding and its efficient
                                                          implementation provide clear benefits: rapid training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and
                                                          hyperparameters across all tasks and only vary the hash table size which trades off quality and performance. Photograph Â©Trevor Dobson (CC BY-NC-ND 2.0)

                                                          Neural graphics primitives, parameterized by fully connected neural net-                           architecture that is trivial to parallelize on modern GPUs. We leverage this
                                                          works, can be costly to train and evaluate. We reduce this cost with a versatile                   parallelism by implementing the whole system using fully-fused CUDA ker-
                                                          new input encoding that permits the use of a smaller network without sac-                          nels with a focus on minimizing wasted bandwidth and compute operations.
                                                          rificing quality, thus significantly reducing the number of floating point                         We achieve a combined speedup of several orders of magnitude, enabling
                                                          and memory access operations: a small neural network is augmented by a                             training of high-quality neural graphics primitives in a matter of seconds,
                                                          multiresolution hash table of trainable feature vectors whose values are op-                       and rendering in tens of milliseconds at a resolution of 1920Ã—1080.
                                                          timized through stochastic gradient descent. The multiresolution structure
                                                          allows the network to disambiguate hash collisions, making for a simple                            CCS Concepts: â€¢ Computing methodologies â†’ Massively parallel algo-
                                                                                                                                                             rithms; Vector / streaming algorithms; Neural networks.
                                                          Authorsâ€™ addresses: Thomas MÃ¼ller, NVIDIA, ZÃ¼rich, Switzerland, tmueller@nvidia.
                                                          com; Alex Evans, NVIDIA, London, United Kingdom, alexe@nvidia.com; Christoph                       Additional Key Words and Phrases: Image Synthesis, Neural Networks, En-
                                                          Schied, NVIDIA, Seattle, USA, cschied@nvidia.com; Alexander Keller, NVIDIA, Berlin,                codings, Hashing, GPUs, Parallel Computation, Function Approximation.
                                                          Germany, akeller@nvidia.com.
                                                                                                                                                             ACM Reference Format:
                                                          Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.                  Thomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. 2022.
                                                          This is the authorâ€™s version of the work. It is posted here for your personal use. Not for
                                                          redistribution. The definitive Version of Record was published in ACM Transactions on
                                                                                                                                                             Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.
                                                          Graphics, https://doi.org/10.1145/3528223.3530127.                                                 ACM Trans. Graph. 41, 4, Article 102 (July 2022), 15 pages. https://doi.org/10.
                                                                                                                                                             1145/3528223.3530127


                                                                                                                                                                      ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
102:2    â€¢ MÃ¼ller et al.


1       INTRODUCTION                                                              In the following, we first review prior neural network encodings
Computer graphics primitives are fundamentally represented by                  (Section 2), then we describe our encoding (Section 3) and its imple-
mathematical functions that parameterize appearance. The quality               mentation (Section 4), followed lastly by our experiments (Section 5)
and performance characteristics of the mathematical representation             and discussion thereof (Section 6).
are crucial for visual fidelity: we desire representations that remain
fast and compact while capturing high-frequency, local detail. Func-           2   BACKGROUND AND RELATED WORK
tions represented by multi-layer perceptrons (MLPs), used as neural            Early examples of encoding the inputs of a machine learning model
graphics primitives, have been shown to match these criteria (to               into a higher-dimensional space include the one-hot encoding [Har-
varying degree), for example as representations of shape [Martel               ris and Harris 2013] and the kernel trick [Theodoridis 2008] by which
et al. 2021; Park et al. 2019] and radiance fields [Liu et al. 2020;           complex arrangements of data can be made linearly separable.
Mildenhall et al. 2020; MÃ¼ller et al. 2020, 2021].                                For neural networks, input encodings have proven useful in the at-
   The important commonality of the these approaches is an encod-              tention components of recurrent architectures [Gehring et al. 2017]
ing that maps neural network inputs to a higher-dimensional space,             and, subsequently, transformers [Vaswani et al. 2017], where they
which is key for extracting high approximation quality from com-               help the neural network to identify the location it is currently pro-
pact models. Most successful among these encodings are trainable,              cessing. Vaswani et al. [2017] encode scalar positions ğ‘¥ âˆˆ R as a
task-specific data structures [Liu et al. 2020; Takikawa et al. 2021]          multiresolution sequence of ğ¿ âˆˆ N sine and cosine functions
that take on a large portion of the learning task. This enables the use                  enc(ğ‘¥) = sin(20ğ‘¥), sin(21ğ‘¥), . . . , sin(2ğ¿âˆ’1ğ‘¥),
of smaller, more efficient MLPs. However, such data structures rely                                                                        
on heuristics and structural modifications (such as pruning, split-                                  cos(20ğ‘¥), cos(21ğ‘¥), . . . , cos(2ğ¿âˆ’1ğ‘¥) .        (1)
ting, or merging) that may complicate the training process, limit              This has been adopted in computer graphics to encode the spatio-
the method to a specific task, or limit performance on GPUs where              directionally varying light field and volume density in the NeRF
control flow and pointer chasing is expensive.                                 algorithm [Mildenhall et al. 2020]. The five dimensions of this light
   We address these concerns with our multiresolution hash encod-              field are independently encoded using the above formula; this was
ing, which is adaptive and efficient, independent of the task. It is           later extended to randomly oriented parallel wavefronts [Tancik
configured by just two valuesâ€”the number of parameters ğ‘‡ and the               et al. 2020] and level-of-detail filtering [Barron et al. 2021a]. We will
desired finest resolution ğ‘ max â€”yielding state-of-the-art quality on          refer to this family of encodings as frequency encodings. Notably,
a variety of tasks (Figure 1) after a few seconds of training.                 frequency encodings followed by a linear transformation have been
   Key to both the task-independent adaptivity and efficiency is a             used in other computer graphics tasks, such as approximating the
multiresolution hierarchy of hash tables:                                      visibility function [Annen et al. 2007; Jansen and Bavoil 2010].
â€¢ Adaptivity: we map a cascade of grids to corresponding fixed-                   MÃ¼ller et al. [2019; 2020] suggested a continuous variant of the
  size arrays of feature vectors. At coarse resolutions, there is a 1:1        one-hot encoding based on rasterizing a kernel, the one-blob en-
  mapping from grid points to array entries. At fine resolutions, the          coding, which can achieve more accurate results than frequency
  array is treated as a hash table and indexed using a spatial hash            encodings in bounded domains at the cost of being single-scale.
  function, where multiple grid points alias each array entry. Such
                                                                                  Parametric encodings. Recently, state-of-the-art results have been
  hash collisions cause the colliding training gradients to average,
                                                                               achieved by parametric encodings which blur the line between clas-
  meaning that the largest gradientsâ€”those most relevant to the
                                                                               sical data structures and neural approaches. The idea is to arrange
  loss functionâ€”will dominate. The hash tables thus automatically
                                                                               additional trainable parameters (beyond weights and biases) in an
  prioritize the sparse areas with the most important fine scale detail.
                                                                               auxiliary data structure, such as a grid [Chabra et al. 2020; Jiang
  Unlike prior work, no structural updates to the data structure are
                                                                               et al. 2020; Liu et al. 2020; Mehta et al. 2021; Peng et al. 2020a; Sun
  needed at any point during training.
                                                                               et al. 2021; Tang et al. 2018; Yu et al. 2021a] or a tree [Takikawa et al.
â€¢ Efficiency: our hash table lookups are O (1) and do not require
                                                                               2021], and to look-up and (optionally) interpolate these parameters
  control flow. This maps well to modern GPUs, avoiding execution
                                                                               depending on the input vector x âˆˆ Rğ‘‘ . This arrangement trades a
  divergence and serial pointer-chasing inherent in tree traversals.
                                                                               larger memory footprint for a smaller computational cost: whereas
  The hash tables for all resolutions may be queried in parallel.
                                                                               for each gradient propagated backwards through the network, every
We validate our multiresolution hash encoding in four representa-              weight in the fully connected MLP network must be updated, for
tive tasks (see Figure 1):                                                     the trainable input encoding parameters (â€œfeature vectorsâ€), only
(1) Gigapixel image: the MLP learns the mapping from 2D coor-                  a very small number are affected. For example, with a trilinearly
    dinates to RGB colors of a high-resolution image.                          interpolated 3D grid of feature vectors, only 8 such grid points need
(2) Neural signed distance functions (SDF): the MLP learns the                 to be updated for each sample back-propagated to the encoding. In
    mapping from 3D coordinates to the distance to a surface.                  this way, although the total number of parameters is much higher
(3) Neural radiance caching (NRC): the MLP learns the 5D light                 for a parametric encoding than a fixed input encoding, the number
    field of a given scene from a Monte Carlo path tracer.                     of FLOPs and memory accesses required for the update during train-
(4) Neural radiance and density fields (NeRF): the MLP learns                  ing is not increased significantly. By reducing the size of the MLP,
    the 3D density and 5D light field of a given scene from image              such parametric models can typically be trained to convergence
    observations and corresponding perspective transforms.                     much faster without sacrificing approximation quality.

ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                                 Instant Neural Graphics Primitives with a Multiresolution Hash Encoding      â€¢   102:3

                                 (b) Frequency             (c) Dense grid           (d) Dense grid             (e) Hash table (ours)         (f) Hash table (ours)
     (a) No encoding
                             [Mildenhall et al. 2020]     Single resolution         Multi resolution                 ğ‘‡ = 214                       ğ‘‡ = 219




   411 k + 0 parameters             438 k + 0               10 k + 33.6 M             10 k + 16.3 M                 10 k + 494 k                  10 k + 12.6 M
11:28 (mm:ss) / PSNR 18.56     12:45 / PSNR 22.90        1:09 / PSNR 22.35         1:26 / PSNR 23.62             1:48 / PSNR 22.61             1:47 / PSNR 24.58

Fig. 2. A demonstration of the reconstruction quality of different encodings and parametric data structures for storing trainable feature embeddings. Each
configuration was trained for 11 000 steps using our fast NeRF implementation (Section 5.4), varying only the input encoding and MLP size. The number of
trainable parameters (MLP weights + encoding parameters), training time and reconstruction accuracy (PSNR) are shown below each image. Our encoding (e)
with a similar total number of trainable parameters as the frequency encoding configuration (b) trains over 8Ã— faster, due to the sparsity of updates to the
parameters and smaller MLP. Increasing the number of parameters (f) further improves reconstruction accuracy without significantly increasing training time.

   Another parametric approach uses a tree subdivision of the do-                vectors. These are concatenated to form a 16-dimensional (same as
main Rğ‘‘ , wherein a large auxiliary coordinate encoder neural net-               (c)) input to the network. Despite having less than half the number
work (ACORN) [Martel et al. 2021] is trained to output dense feature             of parameters as (c), the reconstruction quality is similar.
grids in the leaf node around x. These dense feature grids, which                   If the surface of interest is known a priori, a data structure such
have on the order of 10 000 entries, are then linearly interpolated, as          as an octree [Takikawa et al. 2021] or sparse grid [Chabra et al.
in Liu et al. [2020]. This approach tends to yield a larger degree of            2020; Chibane et al. 2020; Hadadan et al. 2021; Jiang et al. 2020; Liu
adaptivity compared with the previous parametric encodings, albeit               et al. 2020; Peng et al. 2020a] can be used to cull away the unused
at greater computational cost which can only be amortized when                   features in the dense grid. However, in the NeRF setting, surfaces
sufficiently many inputs x fall into each leaf node.                             only emerge during training. NSVF [Liu et al. 2020] and several
                                                                                 concurrent works [Sun et al. 2021; Yu et al. 2021a] adopt a multi-
    Sparse parametric encodings. While existing parametric encod-                stage, coarse to fine strategy in which regions of the feature grid are
ings tend to yield much greater accuracy than their non-parametric               progressively refined and culled away as necessary. While effective,
predecessors, they also come with downsides in efficiency and versa-             this leads to a more complex training process in which the sparse
tility. Dense grids of trainable features consume much more memory               data structure must be periodically updated.
than the neural network weights. To illustrate the trade-offs and to                Our methodâ€”Figure 2 (e,f)â€”combines both ideas to reduce waste.
motivate our method, Figure 2 shows the effect on reconstruction                 We store the trainable feature vectors in a compact spatial hash table,
quality of a neural radiance field for several different encodings.              whose size is a hyper-parameter ğ‘‡ which can be tuned to trade the
Without any input encoding at all (a), the network is only able to               number of parameters for reconstruction quality. It neither relies on
learn a fairly smooth function of position, resulting in a poor ap-              progressive pruning during training nor on a priori knowledge of the
proximation of the light field. The frequency encoding (b) allows                geometry of the scene. Analogous to the multi-resolution grid in (d),
the same moderately sized network (8 hidden layers, each 256 wide)               we use multiple separate hash tables indexed at different resolutions,
to represent the scene much more accurately. The middle image (c)                whose interpolated outputs are concatenated before being passed
pairs a smaller network with a dense grid of 1283 trilinearly inter-             through the MLP. The reconstruction quality is comparable to the
polated, 16-dimensional feature vectors, for a total of 33.6 million             dense grid encoding, despite having 20Ã— fewer parameters.
trainable parameters. The large number of trainable parameters can                  Unlike prior work that used spatial hashing [Teschner et al. 2003]
be efficiently updated, as each sample only affects 8 grid points.               for 3D reconstruction [NieÃŸner et al. 2013], we do not explicitly han-
    However, the dense grid is wasteful in two ways. First, it allocates         dle collisions of the hash functions by typical means like probing,
as many features to areas of empty space as it does to those areas               bucketing, or chaining. Instead, we rely on the neural network to
near the surface. The number of parameters grows as O (ğ‘ 3 ), while              learn to disambiguate hash collisions itself, avoiding control flow
the visible surface of interest has surface area that grows only as              divergence, reducing implementation complexity and improving
O (ğ‘ 2 ). In this example, the grid has resolution 1283 , but only 53 807        performance. Another performance benefit is the predictable mem-
(2.57%) of its cells touch the visible surface.                                  ory layout of the hash tables that is independent of the data that is
    Second, natural scenes exhibit smoothness, motivating the use                represented. While good caching behavior is often hard to achieve
of a multi-resolution decomposition [Chibane et al. 2020; Hadadan                with tree-like data structures, our hash tables can be fine-tuned for
et al. 2021]. Figure 2 (d) shows the result of using an encoding in              low-level architectural details such as cache size.
which interpolated features are stored in eight co-located grids with
resolutions from 163 to 1733 , each containing 2-dimensional feature

                                                                                           ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
    102:4    â€¢ MÃ¼ller et al.

                                                  1/ğ‘ 0                        ğ¹
              ğ¿ = 2, ğ‘ = 1.5
                                                                           0
                                                                           1                                                                     ğ‘š (y; Î¦)
                                                                           2
                                                                           3
                                                                      ğ‘‡    4
                                      ğ‘™ =1                                 5
                               2              0                                                                        y
                                                                           6
                                                                           7
                                                          ğ‘™ =0
    1/ğ‘ 1                            0                            4                                                        ğ¿ Â·ğ¹
                                                                           0
                                                                           1
                               3              6                            2                                               ğ¸
                                                                           3
                                                                           4
                                                                           5
x                                                                          6
                                                                                                                       ğœ‰
                                                                           7
                                     1                            7

                     (1) Hashing of voxel vertices                        (2) Lookup   (3) Linear interpolation   (4) Concatenation       (5) Neural network

    Fig. 3. Illustration of the multiresolution hash encoding in 2D. (1) for a given input coordinate x, we find the surrounding voxels at ğ¿ resolution levels and
    assign indices to their corners by hashing their integer coordinates. (2) for all resulting corner indices, we look up the corresponding ğ¹ -dimensional feature
    vectors from the hash tables ğœƒğ‘™ and (3) linearly interpolate them according to the relative position of x within the respective ğ‘™-th voxel. (4) we concatenate the
    result of each level, as well as auxiliary inputs ğœ‰ âˆˆ Rğ¸ , producing the encoded MLP input ğ‘¦ âˆˆ Rğ¿ğ¹ +ğ¸ , which (5) is evaluated last. To train the encoding, loss
    gradients are backpropagated through the MLP (5), the concatenation (4), the linear interpolation (3), and then accumulated in the looked-up feature vectors.

    Table 1. Hash encoding parameters and their ranges in our results. Only                         âŒŠxğ‘™ âŒ‹ and âŒˆxğ‘™ âŒ‰ span a voxel with 2ğ‘‘ integer vertices in Zğ‘‘ . We map
    the hash table size ğ‘‡ and max. resolution ğ‘ max need to be tuned to the task.                each corner to an entry in the levelâ€™s respective feature vector array,
        Parameter                                         Symbol                    Value        which has fixed size of at mostğ‘‡ . For coarse levels where a dense grid
                                                                                                 requires fewer than ğ‘‡ parameters, i.e. (ğ‘ğ‘™ + 1)ğ‘‘ â‰¤ ğ‘‡ , this mapping
        Number of levels                                  ğ¿                             16
        Max. entries per level (hash table size)          ğ‘‡                     214 to 224       is 1:1. At finer levels, we use a hash function â„ : Zğ‘‘ â†’ Zğ‘‡ to index
        Number of feature dimensions per entry            ğ¹                              2       into the array, effectively treating it as a hash table, although there is
        Coarsest resolution                               ğ‘ min                         16       no explicit collision handling. We rely instead on the gradient-based
        Finest resolution                                 ğ‘ max             512 to 524288        optimization to store appropriate sparse detail in the array, and the
                                                                                                 subsequent neural network ğ‘š(y; Î¦) for collision resolution. The
    3       MULTIRESOLUTION HASH ENCODING                                                        number of trainable encoding parameters ğœƒ is therefore O (ğ‘‡ ) and
    Given a fully connected neural network ğ‘š(y; Î¦), we are interested in                         bounded by ğ‘‡ Â· ğ¿ Â· ğ¹ which in our case is always ğ‘‡ Â· 16 Â· 2 (Table 1).
    an encoding of its inputs y = enc(x; ğœƒ ) that improves the approxima-                           We use a spatial hash function [Teschner et al. 2003] of the form
                                                                                                                                 ğ‘‘
                                                                                                                                           !
    tion quality and training speed across a wide range of applications                                                         ÃŠ
    without incurring a notable performance overhead. Our neural net-                                                 â„(x) =         ğ‘¥ğ‘– ğœ‹ğ‘–     mod ğ‘‡ ,                  (4)
    work not only has trainable weight parameters Î¦, but also trainable                                                           ğ‘–=1
    encoding parameters ğœƒ . These are arranged into ğ¿ levels, each con-                          where âŠ• denotes the bit-wise XOR operation and ğœ‹ğ‘– are unique,
    taining up to ğ‘‡ feature vectors with dimensionality ğ¹ . Typical values                       large prime numbers. Effectively, this formula XORs the results
    for these hyperparameters are shown in Table 1. Figure 3 illustrates                         of a per-dimension linear congruential (pseudo-random) permuta-
    the steps performed in our multiresolution hash encoding. Each                               tion [Lehmer 1951], decorrelating the effect of the dimensions on
    level (two of which are shown as red and blue in the figure) is inde-                        the hashed value. Notably, to achieve (pseudo-)independence, only
    pendent and conceptually stores feature vectors at the vertices of a                         ğ‘‘ âˆ’ 1 of the ğ‘‘ dimensions must be permuted, so we choose ğœ‹ 1 := 1
    grid, the resolution of which is chosen to be a geometric progression                        for better cache coherence, ğœ‹ 2 = 2 654 435 761, and ğœ‹ 3 = 805 459 861.
    between the coarsest and finest resolutions [ğ‘ min, ğ‘ max ]:                                    Lastly, the feature vectors at each corner are ğ‘‘-linearly interpo-
                                j          k                                                     lated according to the relative position of x within its hypercube,
                         ğ‘ğ‘™ := ğ‘ min Â· ğ‘ğ‘™ ,                             (2)                      i.e. the interpolation weight is wğ‘™ := xğ‘™ âˆ’ âŒŠxğ‘™ âŒ‹.
                                                        
                                     ln ğ‘ max âˆ’ ln ğ‘ min                                            Recall that this process takes place independently for each of the
                          ğ‘ := exp                         .            (3)                      ğ¿ levels. The interpolated feature vectors of each level, as well as
                                             ğ¿âˆ’1
                                                                                                 auxiliary inputs ğœ‰ âˆˆ Rğ¸ (such as the encoded view direction and
    ğ‘ max is chosen to match the finest detail in the training data. Due
                                                                                                 textures in neural radiance caching), are concatenated to produce
    to the large number of levels ğ¿, the growth factor is usually small.
                                                                                                 y âˆˆ Rğ¿ğ¹ +ğ¸ , which is the encoded input enc(x; ğœƒ ) to the MLP ğ‘š(y; Î¦).
    Our use cases have ğ‘ âˆˆ [1.26, 2].
       Consider a single level ğ‘™. The input coordinate x âˆˆ Rğ‘‘ is scaled                              Performance vs. quality. Choosing the hash table size ğ‘‡ provides a
    by that levelâ€™s grid resolution before rounding down and up âŒŠxğ‘™ âŒ‹ :=                          trade-off between performance, memory and quality. Higher values
    âŒŠx Â· ğ‘ğ‘™ âŒ‹, âŒˆxğ‘™ âŒ‰ := âŒˆx Â· ğ‘ğ‘™ âŒ‰.                                                                of ğ‘‡ result in higher quality and lower performance. The memory

    ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                                                                           Instant Neural Graphics Primitives with a Multiresolution Hash Encoding                                    â€¢     102:5
                             Gigapixel image                                                                           SDF                                                                                  NeRF
             50                        ğ‘‡ = 224                                                30                                                  ğ‘‡ = 221
                                                                                                                                        ğ‘‡ = 219
                                                                                                                                                                                35
                             ğ‘‡ = 219




                                                                      MAPE (dB)
 PSNR (dB)




                                                                                                                                                              PSNR (dB)
             40                                                                               25
                                                                                                                                                                                                                     ğ‘‡ = 219         ğ‘‡ = 221
                            ğ‘‡ = 216                                                                                                ğ‘‡ = 214
                                                                                                                                                                                30
             30                                             Pluto                             20                                    Clockwork                                                                     ğ‘‡ = 214
                                                            Mars                                                                    Lizard                                                                                           Lego
                                                            Tokyo                                                                   Bearded Man                                                                                      Ship


                  0              100                      200                                       0            50               100                  150                           0               100                    200              300
                        Training time (seconds)                                                               Training time (seconds)                                                           Training time (seconds)

Fig. 4. The main curves plot test error over training time for varying hash table size ğ‘‡ which determines the number of trainable encoding parameters.
Increasing ğ‘‡ improves reconstruction, at the cost of higher memory usage and slower training and inference. A performance cliff is visible at ğ‘‡ > 219 where
the cache of our RTX 3090 GPU becomes oversubscribed (particularly visible for SDF and NeRF). The plot also shows model convergence over time leading up
to the final state. This highlights how high quality results are already obtained after only a few seconds. Jumps in the convergence (most visible towards the
end of SDF training) are caused by learning rate decay. For NeRF and Gigapixel image, training finishes after 31 000 steps and for SDF after 11 000 steps.

                       Gigapixel image: Tokyo                                                           Signed Distance Function: Cow                                                      Neural Radiance Field: Lego

             40                   ğ¿ = 16         ğ¿ = 32                                        22                                                                                                                ğ¿ = 32
                                                                                                                                                     ğ¿ = 32                     36
                        ğ¿=8                                                                                              ğ¿ = 16                                                                  ğ¿ = 16
                                                                                  MAPE (dB)
 PSNR (dB)




                                                                                                                                                                    PSNR (dB)
             35
                                                                                                              ğ¿=8                                                               35
                                                                                               21                                                                                              ğ¿=8
             30       ğ¿=4                                       F=1                                                                                 F=1                                                                                F=1
                                                                F=2                                                                                 F=2                         34                                                     F=2
                                                                F=4                                       ğ¿=4                                       F=4                                                                                F=4
             25                                                                                20
                                                                F=8                                                                                 F=8                         33       ğ¿=4
                                                                                                                                                                                                                                       F=8
                                  ğ¿=2

                       200         300         400                                                       60                 80                        100                                200               300                 400           500
                       Training time (seconds)                                                                 Training time (seconds)                                                          Training time (seconds)

Fig. 5. Test error over training time for fixed values of feature dimensionality ğ¹ as the number of hash table levels ğ¿ is varied. To maintain a roughly equal
trainable parameter count, the hash table size ğ‘‡ is set according to ğ¹ Â· ğ‘‡ Â· ğ¿ = 224 for SDF and NeRF, whereas gigapixel image uses 228 . Since (ğ¹ = 2, ğ¿ = 16)
is near the best-case performance and quality (top-left) for all applications, we use this configuration in all results. ğ¹ = 1 is slow on our RTX 3090 GPU since
atomic half-precision accumulation is only efficient for 2D vectors but not for scalars. For NeRF and Gigapixel image, training finishes after 31 000 steps
whereas SDF completes at 11 000 steps.

footprint is linear in ğ‘‡ , whereas quality and performance tend to                                                         with equal integer coordinates âŒŠxğ‘™ âŒ‹ are not considered a collision;
scale sub-linearly. We analyze the impact of ğ‘‡ in Figure 4, where we                                                       a collision occurs when different integer coordinates hash to the
report test error vs. training time for a wide range of ğ‘‡ -values for                                                      same index. Luckily, such collisions are pseudo-randomly scattered
three neural graphics primitives. We recommend practitioners to use                                                        across space, and statistically unlikely to occur simultaneously at
ğ‘‡ to tweak the encoding to their desired performance characteristics.                                                      every level for a given pair of points.
   The hyperparameters ğ¿ (number of levels) and ğ¹ (number of fea-                                                             When training samples collide in this way, their gradients average.
ture dimensions) also trade off quality and performance, which we                                                          Consider that the importance to the final reconstruction of such
analyze for an approximately constant number of trainable encoding                                                         samples is rarely equal. For example, a point on a visible surface
parameters ğœƒ in Figure 5. In this analysis, we found (ğ¹ = 2, ğ¿ = 16)                                                       of a radiance field will contribute strongly to the reconstructed
to be a favorable Pareto optimum in all our applications, so we use                                                        image (having high visibility and high density, both multiplicatively
these values in all other results and recommend them as the default.                                                       affecting the magnitude of gradients) causing large changes to its
                                                                                                                           table entries, while a point in empty space that happens to refer to
   Implicit hash collision resolution. It may appear counter-intuitive                                                     the same entry will have a much smaller weight. As a result, the
that this encoding is able to reconstruct scenes faithfully in the                                                         gradients of the more important samples dominate the collision
presence of hash collisions. Key to its success is that the different                                                      average and the aliased table entry will naturally be optimized in
resolution levels have different strengths that complement each                                                            such a way that it reflects the needs of the higher-weighted point.
other. The coarser levels, and thus the encoding as a whole, are                                                              The multiresolution aspect of the hash encoding covers the full
injectiveâ€”that is, they suffer from no collisions at all. However, they                                                    range from a coarse resolution ğ‘ min that is guaranteed to be collision-
can only represent a low-resolution version of the scene, since they                                                       free to the finest resolution ğ‘ max that the task requires. Thereby, it
offer features which are linearly interpolated from a widely spaced                                                        guarantees that all scales at which meaningful learning could take
grid of points. Conversely, fine levels can capture small features due                                                     place are included, regardless of sparsity. Geometric scaling
                                                                                                                                                                                          allows
to their fine grid resolution, but suffer from many collisionsâ€”that is,                                                    covering these scales with only O log (ğ‘ max /ğ‘ min ) many levels,
disparate points which hash to the same table entry. Nearby inputs                                                         which allows picking a conservatively large value for ğ‘ max .

                                                                                                                                           ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
102:6    â€¢ MÃ¼ller et al.


   Online adaptivity. Note that if the distribution of inputs x changes                    The optimal number of feature dimensions ğ¹ per lookup depends
over time during training, for example if they become concentrated                      on the GPU architecture. On one hand, a small number favors cache
in a small region, then finer grid levels will experience fewer colli-                  locality in the previously mentioned streaming approach, but on
sions and a more accurate function can be learned. In other words,                      the other hand, a large ğ¹ favors memory coherence by allowing for
the multiresolution hash encoding automatically adapts to the train-                    ğ¹ -wide vector load instructions. ğ¹ = 2 gave us the best cost-quality
ing data distribution, inheriting the benefits of tree-based encod-                     trade-off (see Figure 5) and we use it in all experiments.
ings [Takikawa et al. 2021] without task-specific data structure
maintenance that might cause discrete jumps during training. One                           Architecture. In all tasks, except for NeRF which we will describe
of our applications, neural radiance caching in Section 5.3, con-                       later, we use an MLP with two hidden layers that have a width
tinually adapts to animated viewpoints and 3D content, greatly                          of 64 neurons, rectified linear unit (ReLU) activation functions on
benefitting from this feature.                                                          their hidden layers, and a linear output layer. The maximum resolu-
                                                                                        tion ğ‘ max is set to 2048 Ã— scene size for NeRF and signed distance
   ğ‘‘-linear interpolation. Interpolating the queried hash table entries                 functions, to half of the gigapixel image width, and 219 in radiance
ensures that the encoding enc(x; ğœƒ ), and by the chain rule its com-                    caching (large value to support close-by objects in expansive scenes).
position with the neural network ğ‘š(enc(x; ğœƒ ); Î¦), are continuous.
Without interpolation, grid-aligned discontinuities would be present                       Initialization. We initialize neural network weights according
in the network output, which would result in an undesirable blocky                      to Glorot and Bengio [2010] to provide a reasonable scaling of ac-
appearance. One may desire higher-order smoothness, for exam-                           tivations and their gradients throughout the layers of the neural
ple when approximating partial differential equations. A concrete                       network. We initialize the hash table entries using the uniform dis-
example from computer graphics are signed distance functions, in                        tribution U (âˆ’10âˆ’4, 10âˆ’4 ) to provide a small amount of randomness
which case the gradient ğœ•ğ‘š(enc(x; ğœƒ ); Î¦)/ğœ•x, i.e. the surface normal,                  while encouraging initial predictions close to zero. We also tried a
would ideally also be continuous. If higher-order smoothness must                       variety of different distributions, including zero-initialization, which
be guaranteed, we describe a low-cost approach in Appendix A,                           all resulted in a very slightly worse initial convergence speed. The
which we however do not employ in any of our results due to a                           hash table appears to be robust to the initialization scheme.
small decrease in reconstruction quality.                                                  Training. We jointly train the neural network weights and the
                                                                                        hash table entries by applying Adam [Kingma and Ba 2014], where
4       IMPLEMENTATION
                                                                                        we set ğ›½ 1 = 0.9, ğ›½ 2 = 0.99, ğœ– = 10âˆ’15 , The choice of ğ›½ 1 and ğ›½ 2 makes
To demonstrate the speed of the multiresolution hash encoding, we                       only a small difference, but the small value of ğœ– = 10âˆ’15 can signifi-
implemented it in CUDA and integrated it with the fast fully-fused                      cantly accelerate the convergence of the hash table entries when
MLPs of the tiny-cuda-nn framework [MÃ¼ller 2021].1 We release the                       their gradients are sparse and weak. To prevent divergence after long
source code of the multiresolution hash encoding as an update to                        training periods, we apply a weak L2 regularization (factor 10âˆ’6 ) to
MÃ¼ller [2021] and the source code pertaining to the neural graphics                     the neural network weights, but not to the hash table entries.
primitives at https://github.com/nvlabs/instant-ngp.                                       When fitting gigapixel images or NeRFs, we use the L 2 loss. For
   Performance considerations. In order to optimize inference and                       signed distance functions, we use the mean absolute percentage
                                                                                                                      |prediction âˆ’ target |
backpropagation performance, we store hash table entries at half                        error (MAPE), defined as         |target | + 0.01 , and for neural radiance
precision (2 bytes per entry). We additionally maintain a master                        caching we use a luminance-relative L 2 loss [MÃ¼ller et al. 2021].
copy of the parameters in full precision for stable mixed-precision                        We observed fastest convergence with a learning rate of 10âˆ’4 for
parameter updates, following Micikevicius et al. [2018].                                signed distance functions and 10âˆ’2 otherwise, as well a a batch size
   To optimally use the GPUâ€™s caches, we evaluate the hash tables                       of 214 for neural radiance caching and 218 otherwise.
level by level: when processing a batch of input positions, we sched-                      Lastly, we skip Adam steps for hash table entries whose gradient is
ule the computation to look up the first level of the multiresolution                   exactly 0. This saves âˆ¼ 10% performance when gradients are sparse,
hash encoding for all inputs, followed by the second level for all                      which is a common occurrence with ğ‘‡ â‰« BatchSize. Even though
inputs, and so on. Thus, only a small number of consecutive hash                        this heuristic violates some of the assumptions behind Adam, we
tables have to reside in caches at any given time, depending on how                     observe no degradation in convergence.
much parallelism is available on the GPU. Importantly, this struc-
ture of computation automatically makes good use of the available                          Non-spatial input dimensions ğœ‰ âˆˆ Rğ¸ . The multiresolution hash
caches and parallelism for a wide range of hash table sizes ğ‘‡ .                         encoding targets spatial coordinates with relatively low dimension-
   On our hardware, the performance of the encoding remains                             ality. All our experiments operate either in 2D or 3D. However, it
roughly constant as long as the hash table size stays below ğ‘‡ â‰¤ 219 .                   is frequently useful to input auxiliary dimensions ğœ‰ âˆˆ Rğ¸ to the
Beyond this threshold, performance starts to drop significantly; see                    neural network, such as the view direction and material parameters
Figure 4. This is explained by the 6 MB L2 cache of our NVIDIA                          when learning a light field. In such cases, the auxiliary dimensions
RTX 3090 GPU, which becomes too small for individual levels when                        can be encoded with established techniques whose cost does not
2 Â· ğ‘‡ Â· ğ¹ > 6 Â· 220 , with 2 being the size of a half-precision entry.                  scale superlinearly with dimensionality; we use the one-blob encod-
1 We observe speed-ups on the order of 10Ã— compared to a naÃ¯ve Python implementation.
                                                                                        ing [MÃ¼ller et al. 2019] in neural radiance caching [MÃ¼ller et al. 2021]
We therefore also release PyTorch bindings around our hash encoding and fully fused     and the spherical harmonics basis in NeRF, similar to concurrent
MLPs to permit their use in existing projects with little overhead.                     work [Verbin et al. 2021; Yu et al. 2021a].

ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                                   Instant Neural Graphics Primitives with a Multiresolution Hash Encoding        â€¢   102:7

       Hash table size: ğ‘‡ = 222                        ğ‘‡ = 222                       ğ‘‡ = 212                ğ‘‡ = 217                ğ‘‡ = 222               Reference




Fig. 6. Approximating an RGB image of resolution 20 000 Ã— 23 466 (469 M RGB pixels) with our multiresolution hash encoding. With hash table sizes ğ‘‡ of 212 ,
217 , and 222 the models shown have 117 k, 2.7 M, and 47.5 M trainable parameters respectively. With only 3.4% of the degrees of freedom of the input, the last
model achieves a reconstruction PSNR of 29.8 dB. â€œGirl With a Pearl Earringâ€ renovation Â©Koorosh Orooj (CC BY-SA 4.0)

5     EXPERIMENTS                                                                 application we investigate in this section. As baseline, we compare
To highlight the versatility and high quality of the encoding, we com-            with NGLOD [Takikawa et al. 2021], which achieves state-of-the-art
pare it with previous encodings in four distinct computer graphics                results in both quality and speed by prefixing its small MLP with a
primitives that benefit from encoding spatial coordinates.                        lookup from an octree of trainable feature vectors. Lookups along
                                                                                  the hierarchy of this octree act similarly to our multiresolution cas-
5.1    Gigapixel Image Approximation                                              cade of grids: they are a collision-free analog to our technique, with
                                                                                  a fixed growth factor ğ‘ = 2. To allow meaningful comparisons in
Learning the 2D to RGB mapping of image coordinates to colors                     terms of both performance and quality, we implemented an opti-
has become a popular benchmark for testing a modelâ€™s ability to                   mized version of NGLOD in our framework, details of which we
represent high-frequency detail [Martel et al. 2021; MÃ¼ller et al.                describe in Appendix B. Details pertaining to real-time training of
2019; Sitzmann et al. 2020; Tancik et al. 2020]. Recent breakthroughs             SDFs are described in Appendix C.
in adaptive coordinate networks (ACORN) [Martel et al. 2021] have                    In Figure 7, we compare NGLOD with our multiresolution hash
shown impressive results when fitting very large imagesâ€”up to a bil-              encoding at roughly equal parameter count. We also show a straight-
lion pixelsâ€”with high fidelity at even the smallest scales. We target             forward application of the frequency encoding [Mildenhall et al.
our multiresolution hash encoding at the same task and converge                   2020] to provide a baseline, details of which are found in Appen-
to high-fidelity images in seconds to minutes (Figure 4).                         dix D. By using a data structure tailored to the reference shape,
   For comparison, on the Tokyo panorama from Figure 1, ACORN                     NGLOD achieves the highest visual reconstruction quality. How-
achieves a PSNR of 38.59 dB after 36.9 h of training. With a similar              ever, even without such a dedicated data structure, our encoding
number of parameters (ğ‘‡ = 224 ), our method achieves the same                     approaches a similar fidelity to NGLOD in terms of the intersection-
PSNR after 2.5 minutes of training, peaking at 41.9 dB after 4 min.               over-union metric (IoU2 ) with similar performance and memory cost.
Figure 6 showcases the level of detail contained in our model for a               Furthermore, the SDF is defined ev-
variety of hash table sizes ğ‘‡ on another image.                                   erywhere within the training volume,
   It is difficult to directly compare the performance of our encoding            as opposed to NGLOD, which is only
to ACORN; a factor of âˆ¼ 10 stems from our use of fully fused CUDA                 defined within the octree (i.e. close
kernels, provided by the tiny-cuda-nn framework [MÃ¼ller 2021].                    to the surface). This permits the use
The input encoding allows for the use of a much smaller MLP than                  of certain SDF rendering techniques
with ACORN, which accounts for much of the remaining 10Ã—â€“100Ã—                     such as approximate soft shadows
speedup. That said, we believe that the biggest value-add of the                  from a small number of off-surface
multiresolution hash encoding is its simplicity. ACORN relies on an               distance samples [Evans 2006], as
adaptive subdivision of the scene as part of a learning curriculum,               shown in the adjacent figure.
none of which is necessary with our encoding.                                        To emphasize differences between the compared methods, we
                                                                                  visualize the SDF using a shading model. The resulting colors are
5.2    Signed Distance Functions                                                  sensitive to even slight changes in the surface normal, which empha-
Signed distance functions (SDFs), in which a 3D shape is repre-                   sizes small fluctuations in the prediction more strongly than in other
sented as the zero level-set of a function of position x, are used in             graphics primitives where color is predicted directly. This sensitivity
many applications including simulation, path planning, 3D mod-                    reveals undesired microstructure in our hash encoding on the scale
eling, and video games. DeepSDF [Park et al. 2019] uses a large
                                                                                   2 IoU is the ratio of volumes of the interiors of the intersection and union of the pair
MLP to represent one or more SDFs at a time. In contrast, when just
                                                                                  of shapes being compared. IoU is always â‰¤ 1 with a perfect fit corresponding to = 1.
a single SDF needs to be fit, a spatially learned encoding, such as               We measure IoU by comparing the signs of the SDFs at 128 million points uniformly
ours can be employed and the MLP shrunk significantly. This is the                distributed within the bounding box of the scene.


                                                                                             ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
102:8   â€¢ MÃ¼ller et al.

            Hash (ours)                   NGLOD           Hash (ours)          Frequency      Frequency     Hash (ours)      NGLOD                  Hash (ours)




                                      22.3 M (params)        12.2 M             124.9 k         124.9 k        12.2 M         16.0 M
                                        1:56 (mm:ss)          1:14               1:32            2:10           1:54           1:49
                                        0.9777 (IoU)         0.9812             0.8432          0.9898         0.9997         0.9998




                                      11.1 M (params)        12.2 M             124.9 k         124.9 k        12.2 M         24.2 M
                                        1:37 (mm:ss)          1:19               1:35            1:21           1:04           1:50
                                        0.9911 (IoU)         0.9872             0.8470          0.7575         0.9691         0.9749
Fig. 7. Neural signed distance functions trained for 11 000 steps. The frequency encoding [Mildenhall et al. 2020] struggles to capture the sharp details on
these intricate models. NGLOD [Takikawa et al. 2021] achieves the highest visual quality, at the cost of only training the SDF inside the cells of a close-fitting
octree. Our hash encoding exhibits similar numeric quality in terms of intersection over union (IoU) and can be evaluated anywhere in the scene. However, it
also exhibits visually undesirable surface roughness that we attribute to randomly distributed hash collisions. Bearded Man Â©Oliver Laric (CC BY-NC-SA 2.0)

                 Feature buffers
                                                                                    Predicted color                                    Real-time sparse path tracer
                                                                                                                  Online
                                                                                                                supervised
                                                   ğ‘š enc(ğ‘¥; ğœƒ ); Î¦
                                                                      
                                                                                                                 training




Fig. 8. Summary of the neural radiance caching application [MÃ¼ller et al. 2021]. The MLP ğ‘š enc(ğ‘¥; ğœƒ ); Î¦ is tasked with predicting photorealistic pixel colors
                                                                                                           

from feature buffers independently for each pixel. The feature buffers contain, among other variables, the world-space position x, which we propose to encode
with our method. Neural radiance caching is a challenging application, because it is supervised online during real-time rendering. The training data are a sparse
set of light paths that are continually spawned from the camera view. As such, the neural network and encoding do not learn a general mapping from features
to color, but rather they continually overfit to the current shape and lighting. To support animated content, training has a budget of one millisecond per frame.

             Multiresolution hash encoding (Ours), ğ‘‡ = 15, 133 FPS                                   Triangle wave encoding [MÃ¼ller et al. 2021], 147 FPS
          Far view              Medium view               Close-by view                          Far view              Medium view                Close-by view




Fig. 9. Neural radiance caching [MÃ¼ller et al. 2021] gains much improved quality from the multiresolution hash encoding with only a mild performance
penalty: 133 versus 147 frames per second at a resolution of 1920Ã—1080px. To demonstrate the online adaptivity of the multiple hash resolutions vs. the prior
triangle wave encoding, we show screenshots from a smooth camera motion that starts with a far-away view of the scene (left) and zooms onto a close-by
view of an intricate shadow (right). Throughout the camera motion, which takes just a few seconds, the neural radiance cache continually learns from sparse
camera paths, enabling the cache to learn (â€œoverfitâ€) intricate detail at the scale of the content that the camera is momentarily observing.

ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                                                 Instant Neural Graphics Primitives with a Multiresolution Hash Encoding      â€¢   102:9

                                              Neural Radiance Field: Lego                                 Ours (MLP)                    Linear            MLP            Reference
            36.5

                                                      ğ‘ neurons = 256
PSNR (dB)




                                 ğ‘ neurons = 128
             36            ğ‘ neurons = 64

                         ğ‘ neurons = 32
                                                                                  ğ‘ layers = 1
            35.5                                                                  ğ‘ layers = 2
                                                                                  ğ‘ layers = 3
                         ğ‘ neurons = 16

                   200           300               400          500         600   700
                                                    Training time (seconds)


Fig. 10. The effect of the MLP size on test error vs. training time (31 000                      Fig. 11. Feeding the result of our encoding through a linear transformation
training steps) on the Lego scene. Other scenes behave almost identically.                       (no neural network) versus an MLP when learning a NeRF. The models
Each curve represents a different MLP depth, where the color MLP has                             were trained for 1 min. The MLP allows for resolving specular details and
ğ‘ layers hidden layers and the density MLP has 1 hidden layer; we do not                         reduces the amount of background noise caused by hash collisions. Due to
observe an improvement with deeper density MLPs. The curves sweep the                            the small size and efficient implementation of the MLP, it is only 15% more
number of neurons the hidden layers of the density and color MLPs from 16                        expensiveâ€”well worth the significantly improved quality.
to 256. Informed by this analysis, we choose ğ‘ layers = 2 and ğ‘ neurons = 64.

                                                                                                 function, which we represent by a similar neural network architec-
of the finest grid resolution, which is absent in NGLOD and does                                 ture as Mildenhall et al. [2020]. We train the model in the same ways
not disappear with longer training times. Since NGLOD is essen-                                  as Mildenhall et al.: by backpropagating through a differentiable ray
tially a collision-free analog to our hash encoding, we attribute this                           marcher driven by 2D RGB images from known camera poses.
artifact to hash collisions. Upon close inspection, similar microstruc-
ture can be seen in other neural graphics primitives, although with                                 Model Architecture. Unlike the other three applications, our NeRF
significantly lower magnitude.                                                                   model consists of two concatenated MLPs: a density MLP followed
                                                                                                 by a color MLP [Mildenhall et al. 2020]. The density MLP maps
5.3            Neural Radiance Caching                                                           the hash encoded position y = enc(x; ğœƒ ) to 16 output values, the
In neural radiance caching [MÃ¼ller et al. 2021], the task of the MLP                             first of which we treat as log-space density. The color MLP adds
is to predict photorealistic pixel colors from feature buffers; see Fig-                         view-dependent color variation. Its input is the concatenation of
ure 8. The MLP is run independently for each pixel (i.e. the model is                            â€¢ the 16 output values of the density MLP, and
not convolutional), so the feature buffers can be treated as per-pixel                           â€¢ the view direction projected onto the first 16 coefficients of the
feature vectors that contain the 3D coordinate x as well as addi-                                  spherical harmonics basis (i.e. up to degree 4). This is a natural
tional features. We can therefore directly apply our multiresolution                               frequency encoding over unit vectors.
hash encoding to x while treating all additional features as auxiliary
                                                                                                 Its output is an RGB color triplet, for which we use either a sigmoid
encoded dimensions ğœ‰ to be concatenated with the encoded position,
                                                                                                 activation when the training data has low dynamic-range (sRGB) or
using the same encoding as MÃ¼ller et al. [2021]. We integrated our
                                                                                                 an exponential activation when it has high dynamic range (linear
work into MÃ¼ller et al.â€™s implementation of neural radiance caching
                                                                                                 HDR). We prefer HDR training data due to the closer resemblance
and therefore refer to their paper for implementation details.
                                                                                                 to physical light transport. This brings numerous advantages as has
   For photorealistic rendering, the neural radiance cache is typ-
                                                                                                 also been noted in concurrent work [Mildenhall et al. 2021].
ically queried only for indirect path contributions, which masks
                                                                                                    Informed by the analysis in Figure 10, our results were generated
its reconstruction error behind the first reflection. In contrast, we
                                                                                                 with a 1-hidden-layer density MLP and a 2-hidden-layer color MLP,
would like to emphasize the neural radiance cacheâ€™s error, and thus
                                                                                                 both 64 neurons wide.
the improvement that can be obtained by using our multiresolution
hash encoding, so we directly visualize the neural radiance cache at                                Accelerated ray marching. When marching along rays for both
the first path vertex.                                                                           training and rendering, we would like to place samples such that
   Figure 9 shows thatâ€”compared to the triangle wave encoding of                                 they contribute somewhat uniformly to the image, minimizing
MÃ¼ller et al. [2021]â€”our encoding results in sharper reconstruction                              wasted computation. Thus, we concentrate samples near surfaces by
while incurring only a mild performance overhead of 0.7 ms that                                  maintaining an occupancy grid that coarsely marks empty vs. non-
reduces the frame rate from 147 to 133 FPS at a resolution of 1920 Ã—                             empty space. In large scenes, we additionally cascade the occupancy
1080px. Notably, the neural radiance cache is trained onlineâ€”during                              grid and distribute samples exponentially rather than uniformly
renderingâ€”from a path tracer that runs in the background, which                                  along the ray. Appendix E describes these procedures in detail.
means that the 0.7 ms overhead includes both training and runtime                                   At HD resolutions, synthetic and even real-world scenes can be
costs of our encoding.                                                                           trained in seconds and rendered at 60 FPS, without the need of
                                                                                                 caching of the MLP outputs [Garbin et al. 2021; Wizadwongsa et al.
5.4            Neural Radiance and Density Fields (NeRF)                                         2021; Yu et al. 2021b]. This high performance makes it tractable to
In the NeRF setting, a volumetric shape is represented in terms of a                             add effects such as anti-aliasing, motion blur and depth of field by
spatial (3D) density function and a spatiodirectional (5D) emission                              brute-force tracing of multiple rays per pixel, as shown in Figure 12.

                                                                                                           ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
102:10   â€¢ MÃ¼ller et al.


Table 2. Peak signal to noise ratio (PSNR) of our NeRF implementation with multiresolution hash encoding (â€œOurs: Hashâ€) vs. NeRF [Mildenhall et al. 2020],
mip-NeRF [Barron et al. 2021a], and NSVF [Liu et al. 2020], which require âˆ¼hours to train (values taken from the respective papers). To demonstrate the
comparatively rapid training of our method, we list its results after training for 1 s to 5 min. For each scene, we mark the methods with least error using gold ,
silver , and bronze medals. To analyze the degree to which our speedup originates from our optimized implementation vs. from our hash encoding, we also
report PSNR for a nearly identical version of our implementation, in which the hash encoding has been replaced by the frequency encoding and the MLP
correspondingly enlarged to match Mildenhall et al. [2020] (â€œOurs: Frequencyâ€; details in Appendix D). It approaches NeRFâ€™s quality after training for just
âˆ¼ 5 min, yet is outperformed by our full method after training for 5 sâ€“15 s, amounting to a 20â€“60Ã— improvement that can be attributed to the hash encoding.

                                      Mic             Ficus          Chair     Hotdog        Materials       Drums        Ship         Lego         avg.
   Ours: Hash (1 s)                   26.09           21.30          21.55     21.63         22.07           17.76        20.38        18.83        21.202
   Ours: Hash (5 s)                   32.60           30.35          30.77     33.42         26.60           23.84        26.38        30.13        29.261
   Ours: Hash (15 s)                  34.76           32.26          32.95     35.56         28.25           25.23        28.56        33.68        31.407
   Ours: Hash (1 min)                 35.92           33.05          34.34     36.78         29.33           25.82        30.20        35.63        32.635
   Ours: Hash (5 min)                 36.22           33.51          35.00     37.40         29.78           26.02        31.10        36.39        33.176
   mip-NeRF (âˆ¼hours)                  36.51           33.29          35.14     37.48         30.71           25.48        30.41        35.70        33.090
   NSVF (âˆ¼hours)                      34.27           31.23          33.19     37.14         32.68           25.18        27.93        32.29        31.739
   NeRF (âˆ¼hours)                      32.91           30.13          33.00     36.18         29.62           25.01        28.65        32.54        31.005
   Ours: Frequency (5 min)            31.89           28.74          31.02     34.86         28.93           24.18        28.06        32.77        30.056
   Ours: Frequency (1 min)            26.62           24.72          28.51     32.61         26.36           21.33        24.32        28.88        26.669


                                                                                        just 15 s of training, and competitive with mip-NeRF after 1 min to
                                                                                        5 min of training.
                                                                                            On one hand, our method performs best on scenes with high
                                                                                        geometric detail, such as Ficus, Drums, Ship and Lego, achieving the
                                                                                        best PSNR of all methods. On the other hand, mip-NeRF and NSVF
                                                                                        outperform our method on scenes with complex, view-dependent
                                                                                        reflections, such as Materials; we attribute this to the much smaller
                                                                                        MLP that we necessarily employ to obtain our speedup of several
                                                                                        orders of magnitude over these competing implementations.
                                                                                            Next, we analyze the degree to which our speedup originates
                                                                                        from our efficient implementation versus from our encoding. To
                                                                                        this end, we additionally report PSNR for a nearly identical ver-
Fig. 12. NeRF reconstruction of a modular synthesizer and large natural
                                                                                        sion of our implementation: we replace the hash encoding by the
360 scene. The left image took 5 seconds to accumulate 128 samples at 1080p
                                                                                        frequency encoding and enlarge the MLP to approximately match
on a single RTX 3090 GPU, allowing for brute force defocus effects. The
right image was taken from an interactive session running at 10 frames per              the architecture of Mildenhall et al. [2020] (â€œOurs: Frequencyâ€); see
second on the same GPU.                                                                 Appendix D for details. This version of our algorithm approaches
                                                                                        NeRFâ€™s quality after training for just âˆ¼ 5 min, yet is outperformed by
                                                                                        our full method after training for a much shorter duration (5 sâ€“15 s),
   Comparison with direct voxel lookups. Figure 11 shows an ablation                    amounting to a 20â€“60Ã— improvement caused by the hash encoding
where we replace the entire neural network with a single linear                         and smaller MLP.
matrix multiplication, in the spirit of (although not identical to)                         For â€œOurs: Hashâ€, the cost of each training step is roughly constant
concurrent direct voxel-based NeRF [Sun et al. 2021; Yu et al. 2021a].                  at âˆ¼6 ms per step. This amounts to 50 k steps after 5 min at which
While the linear layer is capable of reproducing view-dependent                         point the model is well converged. We decay the learning rate after
effects, the quality is significantly compromised as compared to the                    20 k steps by a factor of 0.33, which we repeat every further 10 k
MLP, which is better able to capture specular effects and to resolve                    steps. In contrast, the larger MLP used in â€œOurs: Frequencyâ€ requires
hash collisions across the interpolated multiresolution hash tables                     âˆ¼30 ms per training step, meaning that the PSNR listed after 5 min
(which manifest as high-frequency artifacts). Fortunately, the MLP                      corresponds to about 10 k steps. It could thus keep improving slightly
is only 15% more expensive than the linear layer, thanks to its small                   if trained for extended periods of time, as in the offline NeRF variants
size and efficient implementation.                                                      that are often trained for several 100 k steps.
   Comparison with high-quality offline NeRF. In Table 2, we compare                        While we isolated the performance and convergence impact of
the peak signal to noise ratio (PSNR) our NeRF implementation                           our hash encoding and its small MLP, we believe an additional study
with multiresolution hash encoding (â€œOurs: Hashâ€) with that of                          is required to quantify the impact of advanced ray marching schemes
NeRF [Mildenhall et al. 2020], mip-NeRF [Barron et al. 2021a], and                      (such as ours, coarse-fine [Mildenhall et al. 2020], or DONeRF [Neff
NSVF [Liu et al. 2020], which all require on the order of hours to                      et al. 2021]) independently from the encoding and network archi-
train. In contrast, we list results of our method after training for                    tecture. We report additional information in Section E.3 to aid in
1 s to 5 min. Our PSNR is competitive with NeRF and NSVF after                          such an analysis.


ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                                Instant Neural Graphics Primitives with a Multiresolution Hash Encoding      â€¢   102:11




        (a) Offline rendered reference                    (b) Hash (ours), trained for 10 s                               (c) Path tracer
                                                       Rendered in 32 ms (2 samples per pixel)                Rendered in 32 ms (16 samples per pixel)

Fig. 13. Preliminary results of training a NeRF cloud model (b) from real-time path tracing data. Within 32 ms, a 1024Ã—1024 image of our model convincingly
approximates the offline rendered ground truth (a). Our model exhibits less noise than a GPU path tracer that ran for an equal amount of time (c). The cloud
data is Â©Walt Disney Animation Studios (CC BY-SA 3.0)


6   DISCUSSION AND FUTURE WORK                                                      Microstructure due to hash collisions. The salient artifact of our
  Concatenation vs. reduction. At the end of the encoding, we con-               encoding is a small amount of â€œgrainyâ€ microstructure, most visible
catenate rather than reduce (for example, by summing) the ğ¹ -di-                 on the learned signed distance functions (Figure 1 and Figure 7).
mensional feature vectors obtained from each resolution. We prefer               The graininess is a result of hash collisions that the MLP is unable
concatenation for two reasons. First, it allows for independent, fully           to fully compensate for. We believe that the key to achieving state-
parallel processing of each resolution. Second, a reduction of the               of-the-art quality on SDFs with our encoding will be to find a way
dimensionality of the encoded result y from ğ¿ğ¹ to ğ¹ may be too                   to overcome this microstructure, for example by filtering hash table
small to encode useful information. While ğ¹ could be increased                   lookups or by imposing an additional smoothness prior on the loss.
proportionally, it would make the encoding much more expensive.
  However, we recognize that there may be applications in which                     Generative setting. Parametric input encodings, when used in
reduction is favorable, such as when the neural network is signifi-              a generative setting, typically arrange their features in a dense
cantly more expensive than the encoding, in which case the added                 grid which can then be populated by a separate generator network,
computational cost of increasing ğ¹ could be insignificant. We thus               typically a CNN such as StyleGAN [Chan et al. 2021; DeVries et al.
argue for concatenation by default and not as a hard-and-fast rule. In           2021; Peng et al. 2020b]. Our hash encoding adds an additional layer
our applications, concatenation, coupled with ğ¹ = 2 always yielded               of complexity, as the features are not arranged in a regular pattern
by far the best results.                                                         through the input domain; that is, the features are not bijective with
                                                                                 a regular grid of points. We leave it to future work to determine
  Choice of hash function. A good hash function is efficient to com-             how best to overcome this difficulty.
pute, leads to coherent look-ups, and uniformly covers the feature
vector array regardless of the structure of query points. We chose                  Other applications. We are interested in applying the multireso-
our hash function for its good mixture of these properties and also              lution hash encoding to other low-dimensional tasks that require
experimented with three others:                                                  accurate, high-frequency fits. The frequency encoding originated
(1) The PCG32 [Oâ€™Neill 2014] RNG, which has superior statisti-                   from the attention mechanism of transformer networks [Vaswani
    cal properties. Unfortunately, it did not yield a higher-quality             et al. 2017]. We hope that parametric encodings such as ours can
    reconstruction, making its higher cost not worthwhile.                       lead to a meaningful improvement in general, attention-based tasks.
(2) Ordering the least significant bits of Zğ‘‘ by a space-filling curve              Heterogenous volumetric density fields, such as cloud and smoke
    and only hashing the higher bits. This leads to better look-up               stored in a VDB [Museth 2013, 2021] data structure, often include
    coherence at the cost of worse reconstruction quality. However,              empty space on the outside, a solid core on the inside, and sparse
    the speed-up is only marginally better than setting ğœ‹1 := 1 as               detail on the volumetric surface. This makes them a good fit for
    done in our hash, and is thus not worth the reduced quality.                 our encoding. In the code released alongside this paper, we have
(3) Even better coherence can be achieved by treating the hash                   included a preliminary implementation that fits a radiance and
    function as a tiling of space into dense grids. Like (2), the speed-         density field directly from the noisy output of a volumetric path
    up is small in practice with significant detriment to quality.               tracer. The initial results are promising, as shown in Figure 13, and
                                                                                 we intend to pursue this direction further in future work.
   Alternatively to hand-crafted hash functions, it is conceivable to
optimize the hash function in future work, turning the method into
a dictionary-learning approach. Two possible avenues are (1) de-                 7    CONCLUSION
veloping a continuous formulation of indexing that is amenable to                Many graphics problems rely on task specific data structures to
analytic differentiation or (2) applying an evolutionary optimization            exploit the sparsity or smoothness of the problem at hand. Our
algorithm that can efficiently explore the discrete function space.              multi-resolution hash encoding provides a practical learning-based

                                                                                           ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
102:12   â€¢ MÃ¼ller et al.


alternative that automatically focuses on relevant detail, indepen-                           3480569
dent of the task. Its low overhead allows it to be used even in time-                     David Money Harris and Sarah L. Harris. 2013. 3.4.2 - State Encodings. In Digital
                                                                                              Design and Computer Architecture (second ed.). Morgan Kaufmann, Boston, 129â€“131.
constrained settings like online training and inference. In the context                       https://doi.org/10.1016/B978-0-12-394424-5.00002-1
of neural network input encodings, it is a drop-in replacement, for                       Jon Jansen and Louis Bavoil. 2010. Fourier Opacity Mapping. In Proceedings of the 2010
                                                                                              ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (Washington,
example speeding up NeRF by several orders of magnitude and                                   D.C.) (I3D â€™10). Association for Computing Machinery, New York, NY, USA, 165â€”-172.
matching the performance of concurrent non-neural 3D reconstruc-                              https://doi.org/10.1145/1730804.1730831
tion techniques.                                                                          Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias NieÃŸner,
                                                                                              and Thomas Funkhouser. 2020. Local Implicit Grid Representations for 3D Scenes.
   Slow computational processes in any setting, from lightmap bak-                            In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).
ing to the training of neural networks, can lead to frustrating work-                     Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization.
flows due to long iteration times [Enderton and Wexler 2011]. We                              arXiv:1412.6980 (June 2014).
                                                                                          Derrick H. Lehmer. 1951. Mathematical Methods in Large-scale Computing Units. In
have demonstrated that single-GPU training times measured in                                  Proceedings of the Second Symposium on Large Scale Digital Computing Machinery.
seconds are within reach for many graphics applications, allowing                             Harvard University Press, Cambridge, United Kingdom, 141â€“146.
                                                                                          Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika
neural approaches to be applied where previously they may have                                Aittala, and Timo Aila. 2018. Noise2Noise: Learning Image Restoration without
been discounted.                                                                              Clean Data. arXiv:1803.04189 (March 2018).
                                                                                          Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.
                                                                                              Neural Sparse Voxel Fields. NeurIPS (2020). https://lingjie0206.github.io/papers/
ACKNOWLEDGMENTS                                                                               NSVF/
                                                                                          Julien N.P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro,
We are grateful to Andrew Tao, Andrew Webb, Anjul Patney, David                               and Gordon Wetzstein. 2021. ACORN: Adaptive Coordinate Networks for Neural
Luebke, Fabrice Rousselle, Jacob Munkberg, James Lucas, Jonathan                              Representation. ACM Trans. Graph. (SIGGRAPH) (2021).
Granskog, Jonathan Tremblay, Koki Nagano, Marco Salvi, Nikolaus                           Ishit Mehta, MichaÃ«l Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and
                                                                                              Manmohan Chandraker. 2021. Modulated Periodic Activations for Generalizable
Binder, and Towaki Takikawa for profound discussions, proofread-                              Local Functional Representations. In IEEE International Conference on Computer
ing, feedback, and early testing. We also thank Arman Toorians and                            Vision. IEEE.
Saurabh Jain for the factory robot dataset in Figure 12 (right).                          Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David
                                                                                              Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and
                                                                                              Hao Wu. 2018. Mixed Precision Training. arXiv:1710.03740 (Oct. 2018).
REFERENCES                                                                                Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and
                                                                                              Jonathan T. Barron. 2021. NeRF in the Dark: High Dynamic Range View Synthesis
Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz.                from Noisy Raw Images. arXiv:2111.13679 (Nov. 2021).
    2007. Convolution Shadow Maps. In Rendering Techniques, Jan Kautz and Sumanta         Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari,
    Pattanaik (Eds.). The Eurographics Association. https://doi.org/10.2312/EGWR/             Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion:
    EGSR07/051-060                                                                            Practical View Synthesis with Prescriptive Sampling Guidelines. ACM Trans. Graph.
Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-             38, 4, Article 29 (July 2019), 14 pages. https://doi.org/10.1145/3306346.3322980
    Brualla, and Pratul P. Srinivasan. 2021a. Mip-NeRF: A Multiscale Representation for   Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-
    Anti-Aliasing Neural Radiance Fields. arXiv (2021). https://jonbarron.info/mipnerf/       mamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields
Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hed-          for View Synthesis. In ECCV.
    man. 2021b. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields.              Thomas MÃ¼ller. 2021.                   Tiny CUDA Neural Network Framework.
    arXiv:2111.12077 (Nov. 2021).                                                             https://github.com/nvlabs/tiny-cuda-nn.
Rohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Love-       Thomas MÃ¼ller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan NovÃ¡k.
    grove, and Richard Newcombe. 2020. Deep Local Shapes: Learning Local SDF Priors           2019. Neural Importance Sampling. ACM Trans. Graph. 38, 5, Article 145 (Oct. 2019),
    for Detailed 3D Reconstruction. In Computer Vision â€“ ECCV 2020, Andrea Vedaldi,           19 pages. https://doi.org/10.1145/3341156
    Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International      Thomas MÃ¼ller, Fabrice Rousselle, Alexander Keller, and Jan NovÃ¡k. 2020. Neural
    Publishing, Cham, 608â€“625.                                                                Control Variates. ACM Trans. Graph. 39, 6, Article 243 (Nov. 2020), 19 pages. https:
Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De             //doi.org/10.1145/3414685.3417804
    Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero           Thomas MÃ¼ller, Fabrice Rousselle, Jan NovÃ¡k, and Alexander Keller. 2021. Real-time
    Karras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative                Neural Radiance Caching for Path Tracing. ACM Trans. Graph. 40, 4, Article 36 (Aug.
    Adversarial Networks. arXiv:2112.07945 (2021). arXiv:2112.07945 [cs.CV]                   2021), 16 pages. https://doi.org/10.1145/3450626.3459812
Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. 2020. Implicit Functions in        Ken Museth. 2013. VDB: High-Resolution Sparse Volumes with Dynamic Topology.
    Feature Space for 3D Shape Reconstruction and Completion. In IEEE Conference on           ACM Trans. Graph. 32, 3, Article 27 (July 2013), 22 pages. https://doi.org/10.1145/
    Computer Vision and Pattern Recognition (CVPR). IEEE.                                     2487228.2487235
Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and         Ken Museth. 2021. NanoVDB: A GPU-Friendly and Portable VDB Data Structure For
    Joshua M. Susskind. 2021. Unconstrained Scene Generation with Locally Condi-              Real-Time Rendering And Simulation. In ACM SIGGRAPH 2021 Talks (Virtual Event,
    tioned Radiance Fields. arXiv (2021).                                                     USA) (SIGGRAPH â€™21). Association for Computing Machinery, New York, NY, USA,
Eric Enderton and Daniel Wexler. 2011. The Workflow Scale. In Computer Graphics               Article 1, 2 pages. https://doi.org/10.1145/3450623.3464653
    International Workshop on VFX, Computer Animation, and Stereo Movies.                 Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller,
Alex Evans. 2006. Fast Approximations for Global Illumination on Dynamic Scenes. In           Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger.
    ACM SIGGRAPH 2006 Courses (Boston, Massachusetts) (SIGGRAPH â€™06). Association             2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance
    for Computing Machinery, New York, NY, USA, 153â€“171. https://doi.org/10.1145/             Fields using Depth Oracle Networks. Computer Graphics Forum 40, 4 (2021).
    1185657.1185834                                                                           https://doi.org/10.1111/cgf.14340
Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and                    Matthias NieÃŸner, Michael ZollhÃ¶fer, Shahram Izadi, and Marc Stamminger. 2013. Real-
    Julien Valentin. 2021. FastNeRF: High-Fidelity Neural Rendering at 200FPS.               Time 3D Reconstruction at Scale Using Voxel Hashing. ACM Trans. Graph. 32, 6,
    arXiv:2103.10380 (March 2021).                                                            Article 169 (nov 2013), 11 pages. https://doi.org/10.1145/2508363.2508374
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.           Fakir S. Nooruddin and Greg Turk. 2003. Simplification and Repair of Polygonal Models
    2017. Convolutional Sequence to Sequence Learning. In Proceedings of the 34th             Using Volumetric Techniques. IEEE Transactions on Visualization and Computer
    International Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia)         Graphics 9, 2 (apr 2003), 191â€“â€“205. https://doi.org/10.1109/TVCG.2003.1196006
   (ICMLâ€™17). JMLR.org, 1243â€”-1252.                                                       Melissa E. Oâ€™Neill. 2014. PCG: A Family of Simple Fast Space-Efficient Statistically Good
Xavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep          Algorithms for Random Number Generation. Technical Report HMC-CS-2014-0905.
    Feedforward Neural Networks. In Proc. 13th International Conference on Artificial         Harvey Mudd College, Claremont, CA.
    Intelligence and Statistics (Sardinia, Italy, May 13â€“15). JMLR.org, 249â€“256.          Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-
Saeed Hadadan, Shuhong Chen, and Matthias Zwicker. 2021. Neural radiosity. ACM                grove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape
    Transactions on Graphics 40, 6 (Dec. 2021), 1â€”-11. https://doi.org/10.1145/3478513.


ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                                       Instant Neural Graphics Primitives with a Multiresolution Hash Encoding      â€¢   102:13


   Representation. arXiv:1901.05103 (Jan. 2019).                                        The encoding is thus able to learn smooth, non-zero derivatives for
Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.     all spatial locations x.
   2020a. Convolutional Occupancy Networks. In European Conference on Computer
   Vision (ECCV).                                                                          For higher-order smoothness, higher-order smoothstep functions
Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.     ğ‘†ğ‘› can be used at small additional cost. In practice, the computational
   2020b. Convolutional Occupancy Networks. (2020). arXiv:2003.04618 [cs.CV]
Matt Pharr, Wenzel Jakob, and Greg Humphreys. 2016. Physically Based Rendering:
                                                                                        cost of the 1st order smoothstep function ğ‘† 1 is hidden by memory
   From Theory to Implementation (3rd ed.) (3rd ed.). Morgan Kaufmann Publishers        bottlenecks, making it essentially free. However, the reconstruction
   Inc., San Francisco, CA, USA. 1266 pages.                                            quality tends to decrease as higher-order interpolation is used. This
Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and
   Gordon Wetzstein. 2020. Implicit Neural Representations with Periodic Activation     is why we do not use it by default. Future research is needed to
   Functions. In Proc. NeurIPS.                                                         explain the loss of quality.
Cheng Sun, Min Sun, and Hwann-Tzong Chen. 2021. Direct Voxel Grid Optimization:
   Super-fast Convergence for Radiance Fields Reconstruction. arXiv:2111.11215 (Nov.
   2021).                                                                               B     IMPLEMENTATION DETAILS OF NGLOD
Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek        We designed our implementation of NGLOD [Takikawa et al. 2021]
   Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural
   Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. (2021).     such that it closely resembles that of our hash encoding, only dif-
Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin      fering in the underlying data structure; i.e. using the vertices of
   Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng.
   2020. Fourier Features Let Networks Learn High Frequency Functions in Low
                                                                                       an octree around ground-truth triangle mesh to store collision-free
   Dimensional Domains. NeurIPS (2020). https://bmild.github.io/fourfeat/index.html    feature vectors, rather than relying on hash tables. This results
Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan       in a notable difference to the original NGLOD: the looked-up fea-
   Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, Shahram Izadi,
   and Andrea Tagliasacchi. 2018. Real-Time Compression and Streaming of 4D Per-       ture vectors are concatenated rather than summed, which in our
   formances. ACM Trans. Graph. 37, 6, Article 256 (dec 2018), 11 pages. https:        implementation serendipitously resulted in higher reconstruction
   //doi.org/10.1145/3272127.3275096                                                   quality compared to the summation of an equal number of trainable
Matthias Teschner, Bruno Heidelberger, Matthias MÃ¼ller, Danat Pomeranets, and
   Markus Gross. 2003. Optimized Spatial Hashing for Collision Detection of De-        parameters.
   formable Objects. In Proceedings of VMVâ€™03, Munich, Germany. 47â€“54.                    The octree implies a fixed growth factor ğ‘ = 2, which leads to
Sergios Theodoridis. 2008. Pattern Recognition. Elsevier.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
                                                                                       a smaller number of levels than our hash encoding. We obtained
   Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need.        the most favorable performance vs. quality trade-off at a roughly
   arXiv:1706.03762 (June 2017).                                                       equal number of trainable parameters as our method, through the
Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and
   Pratul P. Srinivasan. 2021. Ref-NeRF: Structured View-Dependent Appearance for      following configuration:
   Neural Radiance Fields. arXiv:2112.03907 (Dec. 2021).                                (1) the number of feature dimensions per entry is ğ¹ = 8,
Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn
   Suwajanakorn. 2021. NeX: Real-time View Synthesis with Neural Basis Expansion.       (2) the number of levels is ğ¿ = 10, and
   In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).                (3) look-ups start at level ğ‘™ = 4.
Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and
   Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks.         The last point is important for two reasons: first, it matches the
   arXiv:2112.05131 (Dec. 2021).                                                       coarsest resolution of our hash tables 24 = 16 = ğ‘ min , and second, it
Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021b.
   PlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.
                                                                                       prevents a performance bottleneck that would arise when all threads
                                                                                       of the GPU atomically accumulate gradients in few, coarse entries.
                                                                                       We experimentally verified that this does not lead to reduced quality,
A    SMOOTH INTERPOLATION
                                                                                       compared to looking up the entire hierarchy.
One may desire smoother interpolation than the ğ‘‘-linear interpola-
tion that our multiresolution hash encoding uses by default.                            C     REAL-TIME SDF TRAINING DATA GENERATION
   In this case, the obvious solution would be using a ğ‘‘-quadratic or                   In order to not bottleneck our SDF training, we must be able to
ğ‘‘-cubic interpolation, both of which are however very expensive                         generate a large number of ground truth signed distances to high-
due to requiring the lookup of 3ğ‘‘ and 4ğ‘‘ instead of 2ğ‘‘ vertices,                        resolution meshes very quickly (âˆ¼millions per second).
respectively. As a low-cost alternative, we recommend applying the
smoothstep function,                                                                    C.1     Efficient Sampling of 3D Training Positions
                                         2
                             ğ‘† 1 (ğ‘¥) = ğ‘¥ (3 âˆ’ 2ğ‘¥) ,                             (5)     Similar to prior work [Takikawa et al. 2021], we distribute some
                                                                                        (1/8th) of our training positions uniformly in the unit cube, some
to the ğ‘‘-linear interpolation weights. Crucially, the derivative of the                 (4/8ths) uniformly on the surface of the mesh, and the remainder
smoothstep,                                                                             (3/8ths) perturbed from the surface of the mesh.
                             ğ‘† 1â€² (ğ‘¥) = 6ğ‘¥ (1 âˆ’ ğ‘¥) ,                            (6)        The uniform samples in the unit cube are trivial to generate using
                                                                                        any pseudorandom number generator; we use a GPU implementa-
vanishes at 0 and at 1, causing the discontinuity in the derivatives                    tion of PCG32 [Oâ€™Neill 2014].
of the encoding to vanish by the chain rule. The encoding thus                             To generate the uniform samples on the surface of the mesh, we
becomes ğ¶ 1 -smooth.                                                                    compute the area of each triangle in a preprocessing step, normalize
   However, by this trick, we have merely traded discontinuities for                    the areas to represent a probability distribution, and store the corre-
zero-points in the individual levels which are not necessarily more                     sponding cumulative distribution function (CDF) in an array. Then,
desirable. So, we offset each level by half of its voxel size 1/(2ğ‘ğ‘™ ),                 for each sample, we select a triangle proportional to its area by the
which prevents the zero derivatives from aligning across all levels.                    inversion methodâ€”a binary search of a uniform random number

                                                                                                  ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
102:14   â€¢ MÃ¼ller et al.


over the CDF arrayâ€”and sample a uniformly random position on                                and the second number to the color MLP. For SDFs, we make two ad-
that triangle by standard sample warping [Pharr et al. 2016].                               ditional changes: (1) we optimize against the relative L 2 loss [Lehti-
   Lastly, for those surface samples that must be perturbed, we add                         nen et al. 2018] instead of the MAPE described in the main text, and
a random 3D vector, each dimension independently drawn from                                 (2) we perturb training samples with a standard deviation of ğ‘Ÿ /128
a logistic distribution (similar shape to a Gaussian, but cheaper to                        as opposed to the value of ğ‘Ÿ /1024 from Appendix C.1. Both changes
compute) with standard deviation ğ‘Ÿ /1024, where ğ‘Ÿ is the bounding                           smooth the loss landscape, resulting in a better reconstruction with
radius of the mesh.                                                                         the above configuration.
                                                                                               Notably, even though the above configurations have fewer param-
   Octree sampling for NGLOD. When training our implementation                              eters and are slower than our configurations with hash encoding,
of Takikawa et al. [2021], we must be careful to rarely generate                            they represent favorable performance vs. quality trade-offs. An equal
training positions outside of octree leaf nodes. To this end, we                            parameter count comparison would make pure MLPs too expensive
replace the uniform unit cube sampling routine with one that creates                        due to their scaling with O (ğ‘› 2 ) as opposed to the sub-linear scaling
uniform 3D positions in the leaf nodes of the octree by first rejection                     of trainable encodings. On the other hand, an equal throughput com-
sampling a uniformly random leaf node from the array of all nodes                           parison would require prohibitively small MLPs, thus underselling
and then generating a uniform random position within the nodeâ€™s                             the reconstruction quality that pure MLPs are capable of.
voxel. Fortunately, the standard deviation ğ‘Ÿ /1024 of our logistic                             We also experimented with Fourier features [Tancik et al. 2020]
perturbation is small enough to almost never leave the octree, so                           but did not obtain better results compared to the axis-aligned fre-
we do not need to modify the surface sampling routine.                                      quency encodings mentioned previously.
C.2      Efficient Signed Distances to the Triangle Mesh                                    E     ACCELERATED NERF RAY MARCHING
For each sampled 3D position x, we must compute the signed dis-                             The performance of ray marching algorithms such as NeRF strongly
tance to the triangle mesh. To this end, we first construct a triangle                      depends on the marching scheme. We utilize three techniques with
bounding volume hierarchy (BVH) with which     we perform efficient                        imperceivable error to optimize our implementation:
unsigned distance queries; O log ğ‘ triangles on average.
                                                                                            (1) exponential stepping for large scenes,
   Next, we sign these distances by tracing 32 â€œstab raysâ€ [Nooruddin
                                                                                            (2) skipping of empty space and occluded regions, and
and Turk 2003], which we distribute uniformly over the sphere using
                                                                                            (3) compaction of samples into dense buffers for efficient execution.
a Fibonacci lattice that is pseudorandomly and independently offset
for every training position. If any of these rays reaches infinity, the
                                                                                            E.1    Ray Marching Step Size and Stopping
corresponding position x is deemed â€œoutsideâ€ of the object and the
                                                                                            In synthetic NeRF scenes, which we bound to the unitâˆšcube [0, 1]    3
distance is marked positive. Otherwise, it is marked negative.3                                                                                               âˆš,
   For maximum efficiency, we use NVIDIA ray tracing hardware                               we use a fixed ray marching step size equal to Î”ğ‘¡ := 3/1024; 3
through the OptiX 7 framework, which is over an order of magnitude                          represents the diagonal of the unit cube.
faster than using the previously mentioned triangle BVH for ray-                               In all other scenes, based on the intercept theorem4 , we set the
shape intersections on our RTX 3090 GPU.                                                    step size proportional toâˆš                       ray Î”ğ‘¡ := ğ‘¡/256,
                                                                                                                       the distance âˆšğ‘¡ along the
                                                                                            clamped to the interval 3/1024, ğ‘  Â· 3/1024 , where ğ‘  is size of
D     BASELINE MLPS WITH FREQUENCY ENCODING                                                 the largest axis of the sceneâ€™s bounding box. This choice of step
In our signed distance function (SDF), neural radiance caching                              size exhibits exponential growth in ğ‘¡, which means that the compu-
(NRC), and neural radiance and density fields (NeRF) experiments,                           tation cost grows only logarithmically in scene diameter, with no
we use an MLP prefixed by a frequency encoding as baseline. The                             perceivable loss of quality.
respective architectures are equal to those in the main text, except                           Lastly, we stop ray marching and set the remaining contribution to
that the MLPs are larger and that the hash encoding is replaced by                          zero as soon as the transmittance of the ray drops below a threshold;
sine and cosine waves (SDF and NeRF) or triangle waves (NRC).                               in our case ğœ– = 10âˆ’4 .
The following table lists the number of hidden layers, neurons per                             Related work. Mildenhall et al. [2019] already identified a non-
hidden layer, frequency cascades (each scaled by a factor of 2 as per                       linear step size as benefitial: they recommend sampling uniformly
Vaswani et al. [2017]), and adjusted learning rates.                                        in the disparity-space of the average camera frame, which is more
                                                                                            aggressive than our exponential stepping, requiring on one hand
 Primitive      Hidden layers        Neurons       Frequencies      Learning rate           only a constant number of steps, but on the other hand can lead to a
 SDF                          8           128                10             3 Â· 10âˆ’4        loss of fidelity compared to exponential stepping [Neff et al. 2021].
 NRC                          3            64                10                 10âˆ’2           In addition to non-linear stepping, some prior methods propose to
 NeRF                       7/1     256 / 256             16 / 4                10âˆ’3        warp the 3D domain of the scene towards the origin, thereby improv-
                                                                                            ing the numerical properties of their input encodings [Barron et al.
For NeRF, the first listed number corresponds to the density MLP                            2021b; Mildenhall et al. 2020; Neff et al. 2021]. This causes rays to
                                                                                            curve, which leads to a worse reconstruction in our implementation.
3 If the mesh is watertight, it is cheaper to sign the distance based on the normal(s) of
the closest triangle(s) from the previous step. We also implemented this procedure, but     4 The appearance of objects stays the same as long as their size and distance from the
disable it by default due to its incompatibility with typical meshes in the wild.           observer remain proportional.


ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
                                                                              Instant Neural Graphics Primitives with a Multiresolution Hash Encoding      â€¢   102:15


In contrast, we linearly map input coordinates into the unit cube              Table 3. Batch size, number of rays per batch, and number of samples per
before feeding them to our hash encoding, relying on its exponential           ray for our full method (â€œOurs: Hashâ€), our implementation of frequency
multiresolution growth to reach a proportionally scaled maximum                encoding NeRF (â€œOurs: Freq.â€) and mip-NeRF. Since the values correspond-
                                                                               ing to our method vary by scene, we report minimum and maximum values
resolution ğ‘ max with a constant number of levels (variable ğ‘ as in
                                                                               over the synthetic scenes from Table 2.
Equation (3)) or logarithmically many levels ğ¿ (constant ğ‘).
                                                                                Method          Batch size      =    Samples per ray       Ã—      Rays per batch
E.2   Occupancy Grids                                                           Ours: Hash           256 Ki               3.1 to 25.7              10 Ki to 85 Ki
To skip ray marching steps in empty space, we maintain a cascade                Ours: Freq.          256 Ki                  2.5 to 9             29 Ki to 105 Ki
of ğ¾ multiscale occupancy grids, where ğ¾ = 1 for all synthetic NeRF             mip-NeRF               1 Mi     128 coarse + 128 fine                        4 Ki
scenes (single grid) and ğ¾ âˆˆ [1, 5] for larger real-world scenes (up to
5 grids, depending on scene size). Each grid has a resolution of 1283 ,
                                                                               prediction [Mildenhall et al. 2020] or through neural importance
spanning a geometrically growing domain [âˆ’2ğ‘˜âˆ’1 + 0.5, 2ğ‘˜âˆ’1 + 0.5] 3
                                                                               sampling [MÃ¼ller et al. 2019] as done in DONeRF [Neff et al. 2021].
that is centered around (0.5, 0.5, 0.5).
   Each grid cell stores occupancy as a single bit. The cells are laid         E.3    Number of Rays Versus Batch Size
out in Morton (z-curve) order to facilitate memory-coherent traver-
sal by a digital differential analyzer (DDA). During ray marching,             The batch size has a significant effect on the quality and speed of
whenever a sample is to be placed according to the step size from              NeRF convergence. We found that training from a larger number
the previous section, the sample is skipped if its grid cellâ€™s bit is low.     of rays, i.e. incorporating more viewpoint variation into the batch,
   Which one of the ğ¾ grids is queried is determined by both the               converged to lower error in fewer steps. In our implementation
sample position x and the step size Î”ğ‘¡: among the grids covering x,            where the number of samples per ray is variable due to occupancy,
the finest one with cell side-length larger than Î”ğ‘¡ is queried.                we therefore include as many rays as possible in batches of fixed size
                                                                               rather than building variable-size batches from a fixed ray count.
  Updating the occupancy grids. To continually update the occu-                   In Table 3, we list ranges of the resulting number of rays per
pancy grids while training, we maintain a second set of grids that             batch and corresponding samples per ray. We use a batch size of
have the same layout, except that they store full-precision floating           256 Ki, which resulted in the fastest wall-clock convergence in our
point density values rather than single bits.                                  experiments. This is 4Ã— smaller than the batch size chosen in mip-
  We update the grids after every 16 training iterations by perform-           NeRF, likely due to the larger number of samples each of their
ing the following steps. We                                                    rays requires. However, due to the myriad other differences across
                                                                               implementations, a more detailed study must be carried out to draw
(1) decay the density value in each grid cell by a factor of 0.95,             a definitive conclusion.
(2) randomly sample ğ‘€ candidate cells, and set their value to the                 Lastly, we note that the occupancy grid in our frequency-encoding
    maximum of their current value and the density component of                baseline (â€œOurs: Freq.â€; Appendix D) produces even fewer samples
    the NeRF model at a random location within the cell, and                   than when used alongside our hash encoding. This can be explained
(3) update the occupancy âˆš bits by thresholding each cellâ€™s density            by the slightly more detailed reconstruction of the hash encoding:
    with ğ‘¡ = 0.01 Â· 1024/ 3, which corresponds to thresholding the             when the extra detail is finer than the occupancy grid resolution,
    opacity of a minimal ray marching step by 1 âˆ’ exp(âˆ’0.01) â‰ˆ 0.01.           its surrounding empty space can not be effectively culled away and
The sampling strategy of the ğ‘€ candidate cells depends on the                  must be traversed by extra steps.
training progress since the occupancy grid does not store reliable
information in early iterations. During the first 256 training steps,
we sample ğ‘€ = ğ¾ Â· 1283 cells uniformly without repetition. For sub-
sequent training steps we set ğ‘€ = ğ¾ Â· 1283 /2 which we partition
into two sets. The first ğ‘€/2 cells are sampled uniformly among
all cells. Rejection sampling is used for the remaining samples to
restrict selection to cells that are currently occupied.

   Related work. The idea of constraining the MLP evaluation to
occupied cells has already been exploited in prior work on trainable,
cell-based encodings [Liu et al. 2020; Sun et al. 2021; Yu et al. 2021a,b].
In contrast to these papers, our occupancy grid is independent from
the learned encoding, allowing us to represent it more compactly
as a bitfield (and thereby at a resolution that is decoupled from that
of the encoding) and to utilize it when comparing against other
methods that do not have a trained spatial encoding, e.g. â€œOurs:
Frequencyâ€ in Table 2.
   Empty space can also be skipped by importance sampling the
depth distribution, such as by resampling the result of a coarse

                                                                                         ACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.
